{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from summarizer import Summarizer\n",
    "from nltk.tokenize import TextTilingTokenizer\n",
    "\n",
    "from matplotlib import pylab\n",
    "\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "import my_func\n",
    "import my_segment\n",
    "import my_keywords\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Извлечение субтитров из видео."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \">> It's my pleasure to\\nwelcome Bhaskar Mitra today.\",\n",
       "  'start': 3.9,\n",
       "  'duration': 3.32},\n",
       " {'text': 'Bhaskar is actually stationed', 'start': 7.231, 'duration': 2.579},\n",
       " {'text': 'in our London office currently.',\n",
       "  'start': 9.811,\n",
       "  'duration': 1.779}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = 'https://www.youtube.com/watch?v=g1Pgo5yTIKg'\n",
    "link_id = link.replace('https://www.youtube.com/watch?v=', '')\n",
    "\n",
    "data = YouTubeTranscriptApi.get_transcript(link_id)\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''.join([data[i]['text']+' ' for i in range(len(data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Сегментация на предложения при необходимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '.' not in text:\n",
    "    text_main = my_func.my_processing(text)\n",
    "    # segmentation with NNsplit, deepsegment, punctuator\n",
    "    text = my_segment.segment(text_main, method_name='NNsplit') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>> It\\'s my pleasure to\\nwelcome Bhaskar Mitra today. Bhaskar is actually stationed in our London office currently. He\\'s been at Microsoft now\\nabout 11 years, is that right? Yes. About 11 years as\\na Scientist in Bing. And somewhere during that time, he switched to being a student, about a year or two ago, still while working at Bing. So, he\\'s working full-time and completing his Ph.D. at UCL, under Emine Yilmaz, who many of us know and have\\ncollaborated with as well. He\\'s become one of the leaders in applying neural models specifically to Information\\nRetrieval problems. He and Nick Craswell have a nice paper if you\\nwould like to read it, that\\'s available on\\nthe Web right now currently under submission\\nto foundations and trends and information\\nretrieval that compare traditional ways\\nof approaching information retrieval models and where their parallels\\nare in the Neural world. And he\\'s going to talk to us about some of his recent work. So with that, I\\'ll have\\nBhaskar take it away. >> Thanks for\\nthe nice introduction. So, today\\'s talk, I\\'m going to be focusing on\\nsome of my research that I\\'ve been doing for\\nthe last couple of years both in the context of my work\\nin Bing as well as, as Paul mentioned,\\nI\\'ve recently become a year and a half back\\na research student at UCL. So, the research in\\nthat context as well. This is not going to be\\na general overview of neural IR, but as Paul already\\nalso plugged in, Nick and I have been\\nworking on a manuscript on an overview of the new neural\\nIR field in recent years. So this is currently\\nunder review for Foundations and Trends\\nin Information Retrieval, but we put up\\na early pre-print online. Nick and I, and also some folks from University\\nof Amsterdam, we\\'ve been doing\\na few tutorials recently. This year early specifically\\nWISDM and SIGIR. So, if you are\\ninterested again on a broad overview of this area, there\\'s also all the\\nslides and materials from those tutorials that\\nare available online. So, having said all\\nthat, now coming back to the stuff that we are actually going to talk about\\ntoday, Neural networks. We\\'ve seen a big penetration of neural networks in many\\ndifferent application areas. The same is true for IR. If you\\'ve been keeping track, for example at SIGIR papers, we have seen a pretty\\nbig spike in terms of the papers that focus on neural models in\\nlast couple of years. And this also\\nhappens to be sort of my personal main focus for my Ph.D. as well\\nas work for Bing. The part that I\\'m really interested in when I talk\\nabout Neural Networks is that if you look at\\nhow neural networks has entered\\nthe different fields, what\\'s also has happened is if you look at Speech or Vision, and all of these other\\napplication areas, each of these\\napplication areas has different requirements\\nand challenges. And they have led\\nto different kind of understanding or innovations in the field of\\nmachine learning. So, that\\'s the area that I really want to get\\ninto where we\\'re thinking about doing machine learning\\ndifferently based on being motivated by\\nthe specific challenges in IR. So, what do I mean by IR? What would a typical\\nIR task look like? So, the most popular\\nIR task that we probably all familiar with\\nis a web search, right? So, it involves you\\nhave a short text query, you have an index of billions\\nof long text documents, and then the search engine\\nfetches you top key documents, and presents it to the user. But when I\\'m talking about\\ninformation retrieval, today, it also includes\\nother IR tasks. So, for example,\\nI would refer to Query Auto-Completion\\nwhere given a prefix, you are trying to help\\nthe user formulate the query by\\ncompleting the query or recommending possible suffixes\\nfor the prefix. I\\'ll also give some examples of IR task where IR task could be like predicting the next\\nquery in a search session. So, I\\'ll take these\\nthree tasks as example for some of the\\ndiscussions that we\\'ll have during this talk. One of the ways I can fit any of this IR task is in\\nthis kind of a crude framework. So, this is very intentionally\\na crude frameworks, so bear with me when I\\ndescribe it this way. But basically, you can\\nthink of all IR task as you have some input text\\nwhich could be a query, it could be a query prefix\\nor something else. And then, you\\'ll have a set of different candidates\\nthat you want to retrieve from or rank within. So, for every candidate\\nand an input pair, and you can have some\\nrepresentation of the input and the candidate or potentially\\na joint representation, and then from\\nthat representation, you estimate relevance. The reason I bring\\nthis framework up is because while we were\\nwriting that paper for FnTIR, this helped us classify how different neural approaches\\nhave tried either influencing learning\\nquery representations, or learning representations\\nof documents or even just applying neural networks just for estimating\\nthe relevance. So, for example, the very popular learning to rank\\nframework the way that would fit into\\nthis visualization is that you have\\nan input query text, you have a document text, and then you\\'re generating\\nmanually designed features from these two to give\\na joint representation, and then you can have a neural network or any\\nother machine learning model of your choice for\\nestimating relevance. The part that I am\\nprimarily interested in is actually the\\nrepresentation learning part. And that\\'s what I\\'m going to be focusing on during this talk. So, but before I talk about specifically Neural\\nModels for IR, I wanted to make\\na slight segway to talk about something that personally\\nis a favorite topic of mine, to just talk about the basics of different kind of\\nvector representations, and specifically, this topic of the notions of similarity in\\ndifferent representations. So this slide is really, really kind of the fundamental. So, everybody here knows it, so, just bear with me as I\\nquickly go through them. So just defining\\na couple of things here, local representations and\\ndistributed representations. So, what do I mean by them? So these are terms\\nthat are pretty popular especially in the neural\\nnetwork literature. So, a local representation or one hand representation\\nis when you have, let\\'s say you want to\\npresent k different items, then you represent\\neach of these items by a vector of length k, where all the positions\\nare zero except for a particular position that\\ncorresponds to that item. So, let\\'s say in\\nthis particular example, if I had a set of\\ndifferent items of which three items were\\nbanana, mango, and dog, then in my one hand\\nrepresentation or local representation,\\nfor banana, you have everything as zero, and then one position that\\nhas a one or a non-zero value. On the other side, if you talk about\\ndistributed representation, this is the idea of distributed\\nrepresentation is value, represent items by a vector. While a particular item\\nis represented by having more than one\\nnon-zero items in your vector. A very simple example\\nof this would be if you think of\\nany kind of feature space. So imagine I had\\na feature space where I had features like\\ndoes this item bark, is it a fruit, is it\\nof a particular shape, does it have a tail,\\nand so on so forth. So if you imagine some kind\\nof a feature space like that, then you could say\\nthat a dog could be represented by something\\nthat barks and has a tail. But as a banana, it could be represented by something that\\'s a fruit and has\\na particular shape. The interesting thing\\nabout distributed representation as opposed to local representation is that the moment you have this kind of a feature representation, you can actually reason about which items are\\nsimilar to each other, based on the features. So, for example, in this space, you can see that banana\\nand fruit are similar because they\\'re both fruits,\\nbanana and mango. And they are different\\nfrom dog because they don\\'t have any\\noverlapping features. But as if you\\'re talking about\\nthe local representation, all of these items\\nare distinct because no vector is similar to\\nanother vector in that space. The important corollary\\nhere to point out is that the moment you\\ndefine a feature space, you are in fact\\nimplicitly or explicitly making a choice of what items\\nare similar to each other. For example, you could easily come up with\\nthe feature space, where let\\'s say\\na banana and a dog might actually be more similar to each other than\\na banana and a mango. So maybe your feature space\\nis about, I don\\'t know, some kind of shape like is\\nit long versus is it round, or something on those lines, or some something\\nbased on color, by which you could actually say that in that feature space, banana and dog is more\\nsimilar than let\\'s say mango. So, my last definition on these slides is\\nabout embeddings. So, you\\'ll hear a lot of, like especially in\\nthe Neural Network, all of the people say embedding\\nis a very popular topic. One of the things that I just\\nwant to highlight is that an embedding is\\na new latent space that retains the properties\\nand relationships between items from\\nan original feature space. So, a lot of the work\\nwhere we talk about, let\\'s say [inaudible] or\\nother kind of feature, other kind of embeddings\\nthat we talked about. One thing to keep in mind is\\nthat all of them are derived from an original potentially\\nsparse feature space, and the embeddings\\nbasically have the same relationship as in\\nthe original feature space. However, your\\nembeddings might be, because of the nature\\nof them being more smaller dimensional and dense, they might be generalizing\\nmore or other factors. But the important thing\\nis that a lot of the relationships that are\\ninteresting properties that you\\'re seeing in your\\nembedding space are actually existed in your original\\nsparse feature space as well. Why is this important? This is important because\\nI personally have found that it\\'s quite difficult to reason about\\nembedding spaces, because of the nature\\nof these dense vectors. But if you think\\nabout the same items and their original\\nsparse feature space, sometimes you can build a lot of other kind of intuitions\\nthat can be used. Well, I\\'ll give some examples\\nof the same thing. Right? So, before I\\ngo into the example, this is the kind of\\nthe question that I wanted to put forward. And this is one of\\nmy favorite topic, so, bear with me if I talk\\nabout this a bit too long. So this is the idea\\nof thinking about, explicitly thinking about\\nwhat kind of relationships are you capturing in\\na certain vector representation? So, let\\'s say you\\'ve\\nlearned an embedding of words or concepts, and then you have Seattle, and the question might arise. Is Seattle similar to Sydney\\nbecause they\\'re both cities? Or should Seattle be similar to a Seahawks because\\nof Seattle Seahawks? So, I want to take\\nsome example and actually answer this question about like how some of the choices\\nyou make in designing, or how you learn\\nyour embeddings would actually influence which of these relationships you\\nwould actually modeling and your embedding space\\nactually captures. So what I\\'m going to do here, is I\\'m going to take\\na really small toy corpus of 16 short sentences. And from those 16\\nshort sentences, I\\'m going to try and\\nderive different kind of representation for\\nthe four domes, Seattle, Denver,\\nBroncos, and Seahawks. And basically, my goal here\\nis to show that depending on how you featurize these words\\nbased on this corpus, you actually end up capturing different kind of relationships\\nbetween these four domes. So, the first thing I\\'m going to do is I\\'m going to featurize these words based on which\\ndocuments they appeared in. So we see that in\\nour document, for example, Seattle appears in\\ndocument one and two, Seattle Seahawks appears\\nin document three. They both appear in document\\nfive, and so on so forth. Just a quick note\\non the colors here. So the white here means a zero, the grey means a non-zero value, and the only reason\\nI have a light and a dark gray is to indicate those columns where you have more than one\\nnon-zero element. So the diagram\\nbasically shows there is some overlap between at least\\ntwo of the vectors in here. So, basically, if you\\'re\\ngoing to squint at this particular diagram\\nhard enough, you\\'d start seeing\\nthat the vector for Seattle based on this very\\ncrude rudimentary example, actually starts looking very similar to the vector\\nfor Seahawks. And similarly, if you look\\nat Denver and Broncos, these start looking\\nsimilar to each other. For people in IR , this immediately\\nwould remind you of the dome document metrics\\nthat you would typically factorize for LDA and\\nother representations, right? So now, let\\'s do a jump and\\ndo a very different kind of featurization based\\non neighboring words. But with one small\\nadditional detail that I\\'m actually going to consider the distance between the two words. So what\\ndo I mean by that? What I mean by that is that I have a feature\\nSeahawks plus one, which means that this is a feature that says that on\\none position to the right, the word Seahawks appears. So for Seattle Seahawks, you have a non-zero value, because somewhere in here, you have a bunch of Seattle Seahawks that\\nyou see in your corpus. So, then, you also have a, let\\'s take the example\\nSeattle Seahawks Wilson. So the word Wilson actually ends up giving you\\ntwo different features, Wilson plus one and plus two. So Wilson plus one means it occurs one position to\\nthe right of this word, which is true for Seahawks. So you will see that\\nWilson plus one actually is a feature for Seahawks,\\nbut not for Seattle. But Wilson plus two, which means it\\'s\\ntwo position to the right, is a feature that\\'s\\nnon-zero for Seattle, but again, it\\'s\\nzero for Seahawks. So you take this feature\\nspace, and now, again, do the same exercise of squinting really\\nharder this diagram, and suddenly you\\nstart seeing that Seattle and Denver\\nstarts looking similar, and Seahawks and Broncos\\nstarts looking similar. Right? So this starts giving\\nyou more of a notion of a tie based similarity\\nin this diagram. By the way, for people in NOP, people refer to this for\\na pretty long time as kind of syntagmatic and\\nparadigmatic relationships on using a different dome, but I mean something\\nvery similar. The third version is similar to the\\nprevious one but we do want change where we no longer consider the distance\\nbetween the words. So now, Wilson is\\na single feature that\\'s true for\\nboth Seahawks and Seattle. And again, if you do\\nthe squinting thing again, what would happen here is now\\nwe actually see that pretty much all four of them has\\nsome kind of an overlap. And like Seattle is\\nsimilar to both Seahawks and to Denver but because of\\ndifferent sets of features. If you think about it, this is exactly the kind\\nof feature space that something like Word2Vec\\nor GloVe operates on. So the point that I\\'m trying\\nto drive towards is that, if you think about\\nthis feature space, you realize that\\nit\\'s the property of which words are\\nsimilar to each other, or the fact that something\\ncan do a vector algebra in the embedding space is actually coming from this\\noriginal Features Space. So this, for example,\\nLevien Goldberg did this work where they showed that the vector algebra like\\nin minus man plus woman, you could actually do this on the sparse feature space\\nwithout even and learning\\nthe embedding stuff. And similarly, it doesn\\'t really matter if you\\'re using metrics factorization on neural nets\\nor some other approach. The relationships\\nyou\\'re modeling is actually much more influenced by the basically what original feature space you\\'re\\ntrying to compress. This by the way, one of\\nthe things about Word2Vec is the window size that you\\nconsider when training Word2Vec. Actually, has\\nsome strong influence on the balance between the type and\\ntopic-based similarity. So, basically if\\nyou think of this, this is the reason why in\\nthe Word2Vec embeddings, you would see Seattle\\nbeing close to both Seahawks and Denver. So, it has a mix of this type\\nand topic similarity. And how much of this type\\nversus topic might depend on the hyperparameters, for example like\\nthe window size and stuff, and which I\\'m not going\\nto go into details but we can talk about why that it is after the talk. I wanted to jump into\\nthis example because this often is pretty intuitive when you look at this\\nvisually like this, but then obviously,\\nthe questions come up, why is this important? So why are we\\ntalking about this in the context of neural IR? This is important\\nespecially because, so if you think\\nabout the IR problem, to be honest this is true for any variable\\nyou use embeddings, not just in IR, but taking the example\\nof an IR task, what you would\\ntypically do is given, let\\'s say an input text\\nlike a query or a prefix, you could come up with some projection into\\nan embedding space. And then your assumption is that the neighbors in\\nthat embedding space are the good results or candidates\\nthat you want to rank high and show to the user. So, what that also means is\\nthat for a particular input, if these are all\\nmy right candidates, then it means that\\nthese right candidates should also be\\nsimilar to each other. Now, if you\\'re talking\\nabout different IR tasks, so we talked about in the beginning like\\ndocument ranking, Query Auto-Completion\\nor related search, the argument that I\\'m\\nreally trying to make is that each of\\nthese different IR tasks expects different notions\\nof similarity between the different items that\\nthey\\'re trying to rank. And it\\'s important to understand what relationship\\nyou\\'ve modelled in your feature space and whether or not that\\'s appropriate for\\nthe task that you are trying to apply to. So, going back to\\nthe same three examples. So, let\\'s achieve\\nmy document ranking example as that the query is\\ncheap flights to London. And let\\'s assume these\\nare the titles of the documents that I\\'m ranking. So cheap flights to\\nLondon obviously, should be relevant to the document\\ncheap flights to London. But then obviously,\\nyou don\\'t want to see documents\\nabout cheap flights to Sydney or hotels\\nin London, right? However, if you take\\nthe Query Auto-Completion task, imagine this is also modelled in a similar way\\nvia given a prefix, I have some prefix embedding that projects into\\na space and then I\\'m doing nearest neighbour\\nsearch based on suffixes. So, I\\'m just ranking suffix. Let\\'s assume that\\'s how we do the Query Auto-Completion\\ntask here. So here, now what happens\\nis if you have the prefix, cheap flights to,\\nyou actually would, as you expect London\\nand Sydney to be there. They are potentially\\nright answers here. So, what that means\\nis that in this space, if you\\'ve learned\\nthe representation, London and Sydney should\\nactually be close to each other. Whereas, they\\ndefinitely should not be close to each other here. On the other hand,\\nLondon and Big Ben should not be close\\nto each other here. Whereas here, you could\\npotentially rank a document high that also\\nmentioned Big Ben. So, this very kind of a topical similarity\\nversus type similarity. And then we\\'ll talk of\\nnext query suggestion. There\\'s a whole lot of\\nother things that opens up. So for example here, the same topic is obviously\\nrelevant but you\\'re also trying to predict\\nthe next step in the task. So here, there\\'s almost like a directionality factor as well. Like you can think of\\ncertain pairs of queries where query B will always follow query A and\\nnot the other way around. So for example, if\\nyou see two queries like the big clock tower\\nin London and Big Ben, you would probably expect\\nthat the big clock tower in London query came\\nbefore Big Ben because it indicates that the user\\ndid not know what it\\'s called. So there\\'s a potentially different kind of\\nrelationship that you would need for doing\\nthis particular task. I\\'m going to stay on\\nthis topic a little longer by showing\\nanother single model trained on three different types\\nof data that demonstrates these three different types\\nof relationships. So this is the very\\npopular, the DSSM model. So again, people here should be pretty\\nfamiliar with this model. The DSSM model is\\na Siamese network. It was originally proposed\\nfor the document ranking task. What this model basically\\ndoes is it takes a query, it takes a document title, it projects the query\\ninto some embedding space of 120 dimensional\\nembedding space, it projects the document title\\nin the same space, and if the query and document\\nare relevant to each other, then you would expect\\nthe distance to be small, the cosine similarity\\nto be high. And the way you train\\nthis model is you take a query, a positive or a relevant\\ndocument for that query and you take a bunch of\\nnegative documents which are not relevant\\nto the same query and you train to optimize cross entropy at\\nthe top to predict the right document from\\nthe negative documents. So now, we take\\nthe same exact model. The model doesn\\'t really\\nspecify what text, like you could\\npresent any pairs of short text here and\\ntrain this model. So, let\\'s take the same model\\nand actually train it on three different\\ntypes of data. So we\\'re going to train this\\non query-document titles, which was originally proposed. We are also going to train\\nit on prefix-suffix pairs. So, we\\'ll take a query, split the query randomly\\nat a word boundary, and then train the model on, given a prefix,\\npredict the suffix. And then the third one is to actually take\\npairs of query from such sessions and\\nthen given a query, you try to predict the next\\nquery in the same session. And basically what\\nyou end up seeing is that if you\\'ve\\ntrained the query, let\\'s take a particular example, let\\'s take the two\\nmodels trained on query-document pair and\\nprefix-suffix pairs. Once you\\'ve finished\\ntraining the model, you take the query model\\nfrom the first set. You project a piece of text in the space and then you look\\nat it\\'s nearest neighbor. And on the other side, you take the query\\nthat was trained on prefixes and you do\\nthe exact same thing. You\\'re given a piece of text, you project it using this model and then you look at\\nits nearest neighbors. You almost immediately see\\nthis pattern especially by the way the models are\\ntrained on short text. Sorry, this was\\nnot for short text. Ignore the last comment. So what you immediately\\nstart seeing is that the model that\\'s trained\\non query-document pair, the nearest neighbor to\\nSeattle is weather Seattle, Seattle weather, Ikea Seattle,\\neverything about Seattle. Whereas, if you\\nlook at the model that\\'s trained on\\nprefix-suffix pairs, it\\'s nearest neighbors are all these other cities Chicago, San Antonio, and Denver,\\nand so on so forth. Similarly, if you project query on the piece\\nof text Taylor Swift, the model that\\'s trained on query-document pairs gives you everything about Taylor Swift. Whereas the model that\\'s\\ntrained on prefix-suffix pairs, it actually gives you everything\\nthat\\'s of the same type as Taylor Swift,\\nlike celebrities. Kind of the very same idea\\nas we were looking at before. >> [inaudible] >> Sure. >> How do you train\\nthe prefix-suffix pairs? >> So, you basically\\ntake query compass. For every query, you\\nrandomly split at a word boundary\\nand then you would pass the prefix here and the suffix here and\\nsome negative suffixes here. So the actual paper\\nthat this one cites, this is where we were looking at suffix prediction\\ngiven prefix for dealing with Auto-Completion for rare prefixes where you actually haven\\'t seen any query\\nwith this prefix. If you still had\\na candidate of, let\\'s say, 100,000 most popular suffixes, then you might still\\nbe able to predict with suffix goes\\nwith the prefix. >> Do you have two different endpoints on the left and right? >> Yes. So, the Siamese network, what\\'s happening\\nis it\\'s actually, you have two different models. So, the weights are not\\nshared between this and this but this is\\nthe same model here. Right? >> Okay. That makes sense. >> One question. >> Yeah. >> If you go back to\\nthe previous slide, it seems like there\\nare two things going on from what you said earlier. One is, the supervision signal\\nof what\\'s related. And then, the other question\\nis the co-occurrence of different approaches\\nthat people often apply. If you take\\nthe embedding, so take some general embedding,\\nembed these things, and now use this revision signal\\nto actually learn, for each of these domains so that you don\\'t\\ncomplete both of them. Go back to the earlier example, if the relationships\\nwere there in embedding, then one thing\\nyou can do is just project to the\\nright relationship. And there is\\na separate question of, are the relationships even\\nthere in the first place? Now, you\\'re getting? How much did you get\\nfrom each of those? >> The other thing\\nis also that you might have learned\\nan embedding with certain relationships\\nthat\\'s almost so orthogonal to the task that\\nyou are trying to learn. So, one example is in Bing, when we were working\\non Autosuggest, somebody took a particular\\nembeddings that was trained, basically to capture\\ntopical similarity, sort of trained on like\\nbasically query and documents. And then, what they\\nwanted to do is they wanted to actually use\\nthat for Auto-Completion. So, what they did\\nwas, they initialized the embeddings based on\\nwhat was pre-trained, and then, they did\\ncontinue training. And they actually realized\\nthat it was worse than randomly initializing\\nthe embeddings. Because what happened is,\\nif you think about it, if Seattle is close to Seahawks\\nand far away from Sydney, but in the Query Auto-Completion\\nwhere somebody says, map off where Seattle and Sydney should\\nbe close together, this embedding had\\nto actually push these two things apart, and pull those two things that were far away close together. Now, if that\\'s your only\\nrepresentation of these items, and you\\'re just trying to\\nlearn a function on top, then it depends on whether the original embedding space has captured the useful information in addition to\\ncapturing other stuff. But it could also be that\\nthe original embedding space is completely the wrong space. That there\\'s no other function\\nyou can apply on top to recover the actual useful information that\\nyou\\'re looking for. So, it depends. And basically, the argument I\\'m trying to make, I was going to have\\nthat slide after this is that, all of this really\\nmatters if you\\'re going to learn representation\\nseparately and then, use it for a different task, which is actually\\npretty practical because a lot of\\ntime you don\\'t have enough data for\\nthe supervision signal to do the learn representations. Representation learning\\ntakes a lot of data. So, that\\'s what\\npeople for example, will train what to work on, the document corpus\\nalone, and then, you might plug in\\nthose embeddings, and doing ranking\\nor something else. So, that\\'s when it\\'s becomes\\nimportant to think about, is this even the right thing? Or if the thing doesn\\'t work, then when you\\'re debugging\\nthen it comes to fact like, maybe there\\'s a disconnect\\nbetween the embeddings I\\'ve learned and the tasks that\\nI\\'m trying to apply it on. Or it gives you ideas\\nabout how to write, learn the right relationships so that it might be more\\nuseful for the task. So, it doesn\\'t matter if you\\'re learning\\neverything and doing, then, this question goes away. The third example\\nI wanted to show was the case where the model was the same DSSM model\\nis now trained on pairs of queries\\nfrom surf sessions. And this again shows very\\ndifferent kind of regularity. So, here, you can actually\\nsee a similar kind of regularities that you\\nmight have noticed. You can probably make\\na connection with like the Word2vec at a term level. So, this basically\\nallows you to do similar kind of\\nvector algebra on short text, like you could do with\\nWord2vec on terms on. Because you see that\\nDenver to Denver Broncos is parallel to San Francisco, whoever the team is, sorry. There you go. Seattle to Seahawks,\\nI should always stick to Seattle because\\nI know the answer. And yeah, so this basically, if you train it on\\nsession query pairs, you can basically then use the same model to do things like University of Washington\\nminus Seattle plus Chicago gives you\\nChicago State University. Obviously, this like any other vector algebra\\nbased solution, obviously is not always\\ngoing to be correct. So, you also have like\\nwrong answers sometime here. But it just kind of shows you that it has\\na similar property. And if you think about it,\\nthese models of training on pairs of query in a session\\nor training on prefix, suffix that actually\\nsimilar to Word2vec, but it\\'s training on neighbors. Whereas the original model, if you think about training from query to document is more like training like an LDA model where it\\'s termed\\nto document ID. So, it\\'s kind of has those, the same intuitions\\nthat pops up. So, yeah, so I skipped, I set prematurely\\ntalked about the site, but this is the main point that you really care\\nabout all of this, if you are going to use\\npre-trained embeddings. If your model just learns here embeddings in C2 for\\nthe task that you\\'re doing, then you can be\\ncompletely ignorant, and let the model learn whatever is the right representation. So, I want to give\\nan example quickly of some, but this could be useful. This was actually a joint work\\nwith Eric Nalisnick. He was an intern here, and Rich Caruana,\\nand Nick Craswell. So, this basically\\nled us to thinking about using word embeddings\\nfor document ranking. So, in document ranking, one of the things that\\nyou could use these embedding for is\\nsort of soft matching. So, for example, let say\\nwe have these two passages, and we want to know which\\nof these two passage is more relevant to\\nthe query Albuquerque. So, there\\'s a lot of hints. If you look at the first\\npassage from the fact that it has words\\nlike metropolitan, population, area, and\\nso on and so forth. So, if you knew at some way you could\\ndetermine the other words you expect in a passage or a document that correlates\\nwith your query terms, and that can be\\na useful signal for relevance. Unlike typical IR\\nmodels where you only count the exact occurrence query terms in the document. And you can say that the\\nsecond passage is not relevant because this passage is\\nactually taken from Wikipedia, described from\\nthe Wikipedia page on Microsoft that just happens to mention Albuquerque in there. So, you take this idea, so what you can then\\ndo is you could, given a query in a document, you could represent\\nall the query terms using the word embeddings\\nthat you have. You can represent\\nall the document terms with the embedding you have. Let\\'s assume you just do a simple centroid\\nof the query term, centroid of the document terms, and you have a simple\\ncosine similarity or a dot product to say if the query and\\ndocument is relevant. Let say that\\'s the\\nsimplest model you can do. Now, the interesting,\\nand this is where thinking about\\nthese relationship explicitly might help you is, so when we started\\nthinking about this, one of the things we\\nsuddenly realized is that, if you look at\\nthe Word2vec model, there\\'s actually two different embeddings that\\nare being learned. So, the Work2vec model,\\nwhat happens is you have an input word represented\\nas a one hand vector, multiplied by a metrics, multiplied by another metrics. This is the bottleneck clear. And then, you\\'re trying to predict the correct\\nneighboring words. What happens here is that, you actually have two\\ndifferent weight matrices. And I\\'m going for\\nthe ease of reference, I\\'m just going to call them the in metrics and the out metrics. So, the IN embeddings\\nand the OUT embeddings. Typically, once\\na Word2vec model is trained, people completely discard one of these matrices and just use the other one\\nas the embedding. Now, if you train your\\nWord2vec model on short text, for example, like queries, turns out looking\\na similarity between two different terms\\nbased on both of their IN embeddings or based on both of\\ntheir OUT embeddings, actually gives you a notion of similarity that is closer\\nto a Type A similarity. So, Yale becomes\\nclose to Harvard, NYU, Cornell, and\\nso on and so forth. However, if you\\nwere to represent one of the terms with IN, the other terms with\\nthe OUT embedding, and then do the dot product\\nor cosine similarity, you end up,\\nthe neighbors of Yale starts becoming faculty, alumni, that is misspelled, anyways,\\norientation, and graduate. It took me two years\\nto realize that is misspelled, that\\'s something. Anyways, for document ranking what you\\'re really\\nlooking for is, if you have the query Yale, you\\'re looking for\\na document that not only has the word Yale in it, but likely also has\\nother words like faculty, alumni, and orientation\\nand so forth. At least, you expect that more than occurrences\\nof Harvard and NYU. Although Harvard and\\nNYU\\'s presence might also indicate\\nsome kind of relevance. So, basically what we\\ndid in this case is we plugged in this intuition into the word embedding\\nmodel we showed before by basically doing\\nsomething very simple. We would use the IN embeddings\\nfor the query terms, and the OUT embeddings\\nfor the document terms, so that when you do\\nthe cosine similarity or the dot product between\\nthe query and the document, you\\'re basically computing\\nan IN-OUT similarity for every query term\\nwith every document term. And that gives\\nyou an improvement in relevance in this context. >> A word, and the output\\nis the next word? >> Yes. So the Word2vec is\\ntypically trained, given a word,\\npredict another word within a given context window. >> Including itself or\\nnot including itself? >> Not including itself. It\\'s basically, let\\'s say, if the window size is\\n10 words on each side, it will try to predict\\neach of the 10 words on either side of\\nthis particular word. >> How does that compare\\nto using the DSSM sector? >> You mean in performance wise? >> Yeah. >> Well, the DESM would\\nperform better just because it\\'s trained\\nspecifically for that task. >> I see. >> Right? And the DESM model is learning a topical notion\\nof similarity. >> Because DESM flips\\nlike you\\'re trying to map that query to\\nthe document pair. So, you are extending\\nthe query in that sense. >> Yes. I should clarify. I mean, the DESM model trained\\non query document pairs. If you look at the DESM model trained on prefix-suffix pairs, it\\'s basically learning\\na similar relationship as Word2vec training on word\\nand neighboring words. Again, you can use\\neven that model. We haven\\'t really\\nworked on this, but there is no reason why\\nyou can\\'t actually train, if you had for\\nexample, small amount of query document pair of data, you can actually\\ntrain the DESM model on prefix-suffix pairs, and use it in\\nthe same IN-OUT way as the DESM thing proposes for\\ndocument ranking as well. >> And in that case,\\nyou\\'re training on query? >> Yes. >> Okay. >> This model was\\ntrained on queries. >> I see. >> Basically, the two\\nsecret things here is that, if you train this model on\\nlong text and have a window, the window of text long enough, then even the IN-IN and OUT-OUT starts becoming\\nmore and more topical, the balance moves towards it. So basically, the reason\\nthis becomes important is, you actually get\\na big benefit by training on query terms as\\nopposed to document text, and for short text, the IN-IN and IN-OUT similarity, actually, the difference\\nbecomes bigger. So, you combine\\nthe two things and then you see a big improvement. But if you were to\\ntrain this model on, as I said long text with\\nlong context window, then you might see\\na less difference on this. >> So, if you built this amounts that give you score of the next word and\\nthe previous word, what happens if you create like ten slots in the sentence, and then run proliferation\\nto fill in the transport. Can you generate\\nsentences that way? >> The Word2vec was originally proposed as a\\nsimplification for RNS, if your goal is only\\nto learn on embeddings. But this basically goes back to the same idea of\\nsequence completion. Right? And depending on a few different things,\\nso for example, you could train Word Embeddings, where the word appearing next to you is different\\nfrom the same word appearing two position\\naway from you. So, you could actually\\ntake that into effect, and again, that would change the relationships\\nyou\\'re learning. But in terms of\\nsequence completion, yeah, this basically goes\\nback to the RNN and all of those kind\\nof word directly. >> [inaudible] RNS, but\\njust using this thing, to generate sentences by simply running a little bit\\nvariation through the slots. >> You could. You just\\nhave to, in that case, also take into context then the position and\\nthe distance but there is... >> Yeah. >> You could come up\\nwith a, I mean, again, if you start doing that slowly, it will start evolving into something that starts\\nlooking like RNN. Basically, the point\\nwith RNN is that I want context of not just the previous or the last three or four words, I want to have\\nlonger and longer context, the whole RNN analyst theme is about how can\\nyou have some notion of the things you\\'ve\\nalready seen in the sequence with\\nthe fixed memory size. >> Okay. [inaudible] to\\nunderstand what RNN is. I\\'m just curious, what will\\nhappen just in any case. >> As opposed to RNN, like you can always\\nhave a model that has, if you knew your sequences\\nare always short, you could come up with a model that always knows that there are at max key items before, and you have an exact memory of what you\\'ve seen in the past, and then you\\'re\\nnot doing reckless, but you actually have\\ndifferent way matrices based on positions\\nfor the lobby. I\\'ve never worked\\nin that space, so, I wouldn\\'t comment\\non what works or what doesn\\'t but I\\'m just\\nsaying the answer. Sorry? >> And you don\\'t know if anybody has tried that, just in common? >> I\\'m pretty sure the\\nNLB space, people have. But again, I would probably\\ndefer the question to somebody who operate\\nin that area. So, moving on further\\nand a bit faster. >> Think about it, what was the data\\nthat\\'s available? >> We actually put OUT\\nthe IN-OUT embeddings data for two point\\nseven million words trained on Bing queries, just publicly we\\nhad released it. >> Is anyone using\\nthat training? >> I got questions from\\ndifferent people, so, I assume some people\\nare, but yeah. This was another work joined\\nwell with Fernando and Nick. So, basically, in\\nthe same spirit of looking at word embeddings, the other question\\nthat came up is that, obviously terms are\\ninherently ambiguous. This is one of\\nFernando\\'s favorite topic, and he\\'s really convinced me that this is a very important\\nthing to think about. He\\'s got to think\\nabout this idea of local and global\\nanalysis, right? So, the point is basically that, let\\'s say you learn\\na representation globally based on\\nyour whole corpus, so then you\\'re\\nbasically learning some sort of a crude\\nrepresentation for every term, especially when the term is ambiguous and can take\\ndifferent meanings. Right? And this is not just like the crude Apple computer\\nversus Apple the fruit, it\\'s also very subtle. For example, the word \"CUT\" could mean different\\nthings, but then, if I know that I\\'m\\nusing the word in the context of gasoline tax, then this \"CUT\" means\\nlike cutting tax or reducing deficit or whatever. Basically, the idea was like, \"How can you get really\\nextreme in thinking about a local sense of a term, or a representation of a term?\" What we did is, we\\ntried this experiment in the area of query expansion\\nfor retrieval. What we would do\\nis, given a query, we would send\\nthat query to the index, and get a bunch of results, and then we would at\\nrun time train a work to work model on that corpus of\\ntop thousand documents, and then use that embedding\\nto expand the original query, send it back to the corpus\\nand get results. And basically, the point\\ncomes down to is this. What this diagram shows is, the blue dot in the center\\nis the query centroid, and the way you are\\nexpanding the query is by finding other terms that are\\nsimilar to the query center. So basically, you\\'re\\ndoing a nearest neighbor around the space. These are just contour lines, but every other circle\\nhere are the terms candidate terms that you\\ncan use to expand the query. And the red terms are\\nthe actual good terms, that we know that if we\\nadd this to the query, you would see\\nan increase in NDCG. And that\\'s what it looks like for a global word vector space, and this is what it looks like\\nwith a local vector space. But the idea here\\nmainly is that, I think this actually relates to a slightly broader idea that, for any kind of\\nmodeling, for example, when you\\'re learning\\na query document ranking model, all these Neural net models are basically starting to encode real world information\\nabout correlations, right? So, if somebody says,\\nlet\\'s say Albuquerque. Right? So you expect that\\nfor the word Albuquerque, you would expect\\nthese things like altitude or population or area, as to be the\\ncorrelated words that you would expect\\nin the document. But these models\\nare always going to be inferior to a very, very narrow topic, like something specifically\\nin microbiology. It\\'s very hard that this model can memorize those correlations. And the idea was\\nthat, we can think of actually training models\\npotentially at run time, or doing things differently, such that these models\\ncan still go in and effectively research\\na particular topic, learn a new representation and then apply it to a problem. The metaphor that\\nI use with Nick, is thinking about whether your model is\\na librarian or a library. Can your model practically learn everything about everything in the whole universe\\nand be a library? Or is it like a smart librarian who might not know\\nenough, but knows, given a question\\non microbiology, that that\\'s the section\\nin microbiology, let\\'s go there, let\\'s\\nread up all the books, learn a new representation. Now, I know which book to\\nlook at, and so on so forth. So, that was kind of the idea, but this was a crude way\\nof doing this. >> The local embedding is\\ntrained on different area. >> The local embedding\\nis, given a query, it actually retrieves\\na top thousand documents. And the embedding is trained\\non those thousand documents. >> The local one has not\\nbeen trained from that data. >> It has been trained on a random sample of documents\\nfrom the same compass, but not for the query specific. >> [inaudible] seem\\nthe same there. >> Yeah.\\n>> So, this is like, because usually\\nwhen they talk about this bearing\\nthey\\'re saying well, all the information is\\nthere, you just have to just find the right projection. >> Yes, but the whole... >> In this case it\\'s just, it\\'s actually\\ndifferent data, so, you may not even be able\\nto find the projection. >> So the point is, even\\nif your corpus is fixed, it\\'s almost impossible\\nfor you to learn on everything in\\nthe corpus within. So, this also goes\\nback to thinking about machine learn model having\\ncertain amount of capacity. And it\\'s going to\\nprioritize learning things well that\\nit sees more often, and not learn those things\\nthat it sees rarely. Whereas if you knew\\nall you wanted to do was to answer this question\\nabout microbiology, you could go find only those thousand documents\\non microbiology. So, there\\'s enough data,\\nbut the model basically doesn\\'t prioritize if it has\\nto learn the whole universe. But then, it can go and learn that small universe much more efficiently, and\\nbe more effective. >> This is hard to\\ntell, because it\\'s a very high\\ndimensional embedding. So, you have space\\nfor everything. >> Technically, yes. >> The difference may be\\nin a very [inaudible]. >> It could be.\\nSo there might be some thing that you\\ncould do to force this. There maybe others\\nsolution to this, but yeah. >> So, let me come back to the question\\nin different data. I mean, the local is just a sub-sample of\\nthe global, right? So, with that different data\\nit\\'s a subsample. >> No, it\\'s different.\\nIt\\'s actually. >> It\\'s a different sample.\\nIt\\'s not different data, it\\'s not different documents, it\\'s a subsample\\nof a subsample of. >> But they are\\ndifferent samples. >> There different samples, they are not\\ndifferent data, right? >> Well. >> If everything was\\nlearnable from the global, then you would still\\nhave the relationships that global has access\\nto everything local. >> If the global is big\\nenough to get the samples. If in general all the data. >> Local is a subset of global. >> And what if you sample?\\nIf you sample you will not get anything that\\'s\\nin the datas. >> You can go global\\nover everything. >> It trends\\neverything but it is sub-sampling from the overall\\ndistribution because... >> So initialize\\nthe seeds and you can do a sub-sample\\nrelationship. Same thing is done on\\nthe learning curve. >> Basically the idea is that global might have\\na lot of noise. So some of the\\nsample relationship may be drowned out by noise, so and then kind of buy like two level ranking where\\nyou use some sort of naive textural benching\\ntype of stuff to discover lightly relevant subset\\nand then you hope to discover locally stronger\\nrelationship from that subset. Is that, kind of, reasonable\\nway to look at it? >> Right. So basically\\nyour global, the model, it\\'s any model that\\nyou train on somebody that\\'s going to prioritize learning things\\nthat it sees often. But then you can use for example the global model\\nto get some ranking of basically narrow down which domain you\\nreally, this belongs to. And then you can have\\na local model that\\'s trained on that domain\\nspecific data. And then it would likely perform better on that specific topic. >> So if I was to put,\\nyou said previously, for example the\\nglobal corporates you will have many fewer documents\\non like leverage. Then you\\'ll sample\\nthese slots on purpose. So you don\\'t see the same data\\nin the two cases. >> Yes. It\\'s true. Having said that I would still\\nimagine that if you did train on the exact same\\nsamples but like subsample, I think this would still hold. But what you described\\nis what we write instead. >> If you just take the global data and then\\nedit the local sample to it. Now the question is, those are two different questions right? Would you then have a problem\\nwith learning and still ignore the data just because, but in this case you are not\\neven seeing it, so then... >> Potentially yes. >> Yeah. >> Okay. So I\\'m going to go\\ninto the last part of this talk where this is more recent work\\nwhere we actually started looking at\\ndeep neural nets trained end to end so no longer thinking explicitly of these representations\\nand all that stuff. But having deep neural\\nnets and kind of a very clean set up and training them for\\ndocument ranking. And obviously this is\\nall based on the fact that we have a lot\\nof data in Bing so we can actually train these models end to end on our ranking data. So what we started\\nlooking at was the kind of a classical document\\nretrieval task where you have a short-text qwerty\\nin a long text documents. A lot of the models in IR\\ntoday actually looked at short-text verses\\nshort-text the book. And one of the things to kind of realize is the challenges of doing short-text to long-text are different,\\nboth good and bad. So, short-text to short-text, this embedding based models actually benefit\\na lot from the fact that short-text to short-text the vocab mismatch problem\\nis much more severe. So if your model understands synonymy or related things\\nit\\'s most likely to show you improvement over exact matching\\nbased similarities. On the other hand for long- text the opportunities lies from\\nthe fact that long text has mixture of many topics\\nand the matches could be actually in different parts of the document and based on which part of\\nthe document matters, it might indicate\\nmore or less relevance and also things like\\nterm proximity and all of those other things\\nbecomes important. So, there\\'s a lot of\\npossibility of modeling other stuff when\\nyou\\'re dealing with long-text not just\\nthe vocab mismatch. An interesting way\\nof thinking about this neural nets for IR, is to take these two\\nparticular queries. So these are my favorite\\nposter child queries, Pekarovic land company and what channel are\\nthe Seahawks on today. So the point that I want\\nto make with this query is, Pekarovic land company if\\nthis is the term Pekarovic, you\\'ve never seen in\\nyour training data, then I don\\'t care how you, have your neural model\\ninput representation. So, for example it could be 100 presentations of terms or character trigrams\\nand whatnot. Your model will never going\\nto have a good representation for the term Pekarovic. So even for DSSN\\nthat takes character trigram based inputs you would expect that for a term\\nit has never seen before, it would probably have\\nsome sort of a random projection or not so very informative protection\\nlet\\'s put it that way. So the point is\\nthat for this query, for an embedding based model\\nit\\'s actually, it really struggles with\\na query like this right? On the other hand we\\nknow that typical, a classic IR models actually really do well\\nwith this because if there are like 50 documents in your a billion index\\nthat has this term, then fetch those 50 documents, rank them based\\non where this term appears and how they appear next to each other and so forth. But you could learn\\nall of the same functions within neural net. On the other hand,\\nyou have the query what channel are\\nthe Seahawks on today, here lexical matching model\\nwould actually suffer because the right document potentially doesn\\'t even have\\nthe word channel in it. It probably has actually terms like ESPN or Sky Sports right? So it\\'s almost\\nlike a translation model kind of thing\\nhappening here. If you see these words\\nin the query you expect these other words\\nin the document. So this is where\\nan embedding this model is likely to really shine. And one of the\\nfirst work we did in the space was this thing we call the Duet Architecture\\nwhich was basically just the simple idea\\nthat, hey why not, if these are the two\\ndifferent aspects that might be important in IR why not use a neural net\\nto learn both of them and learn them jointly. So, that basically led us to\\nthis idea of the Duet model. Now this is an architecture\\nwe proposed but then the idea of the Duet goes\\nbeyond just this architecture. You can come up with\\nother architectures that has the same Duet property that\\nyou explicitly are mapping, learning the lexical\\nmatching as well as the semantic matching signal\\nfor retrieval. And the final model in\\nour case was basically a simple linear combination, and you\\'re learning\\nthis whole thing as a single model jointly\\non your supervised data. Well, basically it has\\nquery document and labels. So, training like\\ntypical learning to rank models are based\\non pairwise loss. So, how do these things differ? Right? So the local model\\nit\\'s trying to learn how to do lexical\\nmatching, exact matching. It has no interest in learning any kinds of representation. So the input representation\\nis such that it can not even learn any kind of\\nembeddings in this case. So what\\'s the input\\nrepresentation look like. So these are really simple\\nmatrix representation. So think about a query that has four words and a document\\nthat has thousand words. So the input representation for this sub model is\\na query words by document words matrix which is a binary matrix and it\\'s non-zero every time the word\\nmatches the document. So this means that for example, if you look at this line, slightly shifting line it\\nmeans that you actually saw the occurrence of big deal derby as a phrase in that document. And if you look at these\\ndocuments here, these examples, it\\'s almost visually\\nsuddenly becomes clear which documents are\\nrelevant and which are not. So the relevant documents\\nhas more matches. It has more matches towards the beginning of the document. It has more phrasal matches. It definitely has matches\\nfor all the words in the query unlike this guy\\nand so on so forth. So you can see\\nthese visual patterns from these matrix that then\\nyou can imagine that you can train a neural net\\non top to basically learn what are the good\\npatterns of matches. >> So you are like\\nignoring total frequency completely in\\nthis representation? >> Term frequency, no but inverse document\\nfrequency, yes. Term frequency you\\ncan get by actually just counting along this line. Right? So if the neural model can just count\\nthe number of lines in here and it knows the term frequency for\\neach of these terms. So what we did is, again, you can have different kind\\nof architectures but the simplest thing we did is we had a convolutional model on top that one of the\\nimportant thing was to have the window of the same length\\nas the document. Basically [inaudible]\\nimportant thing is for it to map how early in the document\\nyou see the occurrence, that was an important thing. But other than\\nthat there we tried different things that didn\\'t make too much difference\\nbut I\\'m sure there are things here that you can explore but\\nbasically you\\'re doing a convolution model\\nthat runs over that same matrix that I showed and then it has some more fully connected layers and\\nso on and so forth, then it gives you\\na single score. >> In the line of the document, then there is no convolution? >> It moves along\\nthe query direction. It\\'s moving along\\nthe query direction. So, on the other side. >> About the query\\nand the action? >> Yes. >> There\\'s something that worked on proximity\\njust because of time. I wanted too may be go faster and then we\\ncan come back to the questions but I\\'m skipping some things about proximity\\nand the query, sorry. So now on the distributors\\nmodel, what it does? So this again, if you\\'re\\nfamiliar with the DSSM work. This model is similar\\nto the DSSM with some changes just to\\nspecifically deal with long text. So what this model does, is it takes the query and\\nthen projects the query into an embedding and\\nthen it takes a document. But, instead of projecting\\nthe whole document in the embedding space it\\nactually takes a window over the document and predicts each window to\\nan embedding space. Compares the query\\nembedding with the window embeddings\\nand then it aggregates all the information on the top and gives you\\na single score, right? So, this is a very simple way\\nin describing this model. For more details you\\ncan always look at the paper. So yes. So basically, we\\nthen trained it in a very similar way\\nas the DSSM model. You would have a query\\nof positive document and a bunch of\\nnegative documents. You train it to maximize\\ncross entropy loss. We have a bunch of\\ndifferent findings but one of the main things\\nwe found is that, and this was kind of expected is that\\nthe part that really takes a lot of training data to learn is that it,\\ntext representation. So the local model or the lexical side of\\nthe model actually doesn\\'t benefit too much from being thrown a lot of\\ntraining data at it. Whereas the part that focuses on learning embeddings is\\nthe part that actually. So, for our first Duet paper, we didn\\'t even converge. We basically trained it for like 24 hours or 36 hours and kind of stopped\\nat that point. But basically, if you\\nlook at the training curve it looks like it\\nwas still going up. So, we could have had a lot more better MBCG\\'s by\\ntraining it longer. And the only reason we\\ndidn\\'t is because of time. We later ran the same model on TREC Complex Answer\\nRetrieval Task but the data was smaller and\\nmy training time was faster. And there, we actually\\ngot to a point where it kind of\\nflattened off at something around like\\n32 million samples and so on. And this is one of\\nthe important points, why if you look at a lot of\\npapers in academia today. It\\'s kind of unfortunate\\nbut a lot of focus is on Lexicon Matching Model\\njust because of the lack of big datasets for training\\nthese Supervised Models. I\\'m going to skip this slide. This is also out there. The implementation is out\\ntheir publicly on GitHub. But this let us then\\nthis is the kind of the last recent work I\\nwanted to make a mention of. So, we had another intern Ahmed who came in from\\nUMass earlier this year. And he looked at again, Deep Neural Models given\\nwe had a lot of data. It was something he\\nwas interested in to again look at\\nDeep Neural Models for IR. And the problem we\\nlooked at is the fact that it\\'s not just documents\\nand web searches, not just body text\\nbut it\\'s also title, and URL and Anchor Text and Clickstreams and\\nall the other kind of data sets that\\nwe have, right? So, this work was basically\\nkind of the same kind of you can compare it with\\nlike going from BM25 to BM25F. So basically, how can you use a Neural Model that when\\nyou think of document as a single piece of text but my normal document is\\ncomposed of multiple fields. So, how do you have a Neural Model that actually can make use of multiple fields? So this is a wisdom paper 2018. So basically, the idea is this, you have a document that\\nhas Title Text, URL, body, Incoming Anchor Text, and Incoming Query Text. And what you\\'re doing here. I\\'m going to just give\\nyou a quick overview. So basically, I\\'m\\ngoing to touching up on the main design decisions\\nthat makes a big difference. So, you learn an embedding for each of the Metastreams\\nseparately because each of these fields\\nof Metastreams might have different properties. So you want to learn different\\nembedding spaces for them. For fields that have\\nmultiple instances. So for example, for Anchor Text\\nand Query Text you have multiple clicked queries\\nassociated with this document and multiple Anchored Text\\nassociated with this document. You are effectively doing\\nan average pulling but making sure you take care of how many instances are there\\nand do the right averaging. So, by doing all of\\nthat you end up with these field embeddings and then you learn different\\nquery embeddings to match with each of\\nthese different fields. And this comes from\\nthe fact that for example, the query embedding you want to learn to match with\\ntitle might be different from the query embedding\\nthat you want to learn to match with URL because they captured\\ndifferent properties. Once you\\'ve done\\nthis hadamard or element wise. Product between these\\ntwo embeddings you end up with a match vector corresponding\\nto each of the fields. So this comes from\\nthe intuition from like same similar\\nintuition as BM25F, where you don\\'t want to combine the different fields based\\non individual scores. So this is the same idea\\nthat you could end of let\\'s say the query was\\nBarack Obama Facebook. By having a vector and\\nnot a single score it allows the model to realize that maybe Barack Obama\\nmatched on every field, but Facebook never matched\\non any of the field, versus Barack Obama matched on this field and Facebook\\nmatched on the URL field. So, by having\\na vector it can encode that information but it wouldn\\'t if you had\\na single score there. And then you have\\nsome fully connected layers on top to combine those. And one of the things that\\'s\\nimportant in this context is and this happens even for typical learning\\nto rank model, is that if you have\\na very strong precise signal such as Clickstreams, it can actually hamper while\\nyou learn the other fields. So, one of the things that\\nwe found really useful especially in\\nthe presence of a field like click data is to have a field level\\ndrop out in this model. So yes. So that\\'s basically it. So basically, one\\nof the things on a concluding note\\nwanted to say was, these Deep Neural\\nModels obviously there\\'s a lot of\\nexcitement in the field. And people have been thinking about like how this is going to influence this\\nIR field in general. My personal observation is that all of the stuff\\nthat we typically talked of IR initially\\nwent into designing of features during the learning\\nto rank frame days. And now, when we\\'re starting to talk about\\nthese Deep Neural Nets. The same intuitions, scored\\nintuitions from IR is actually fueling the design of these different\\narchitectures. Just this one I\\'m not going to talk about what\\nwe\\'ve also learned recently some work on\\nproactive recommendations such as pro actively, recommending\\nattachments for email. And yes, these are\\nall the papers that I covered. And thank you. Sorry for\\ngoing a bit over time. >> So we have time for questions\\nfor Professor. [inaudible] >> Those commercial\\nnetwork filters, what they looked like, I\\ndidn\\'t quite understand how you built that up. But it seems like\\nit would be very interesting to see\\nwhat these filters are- >> For the local model, the disclaimer is we\\ndidn\\'t spend a lot of time doing too many experiments. But one of the things\\nwe did find was to have the length\\nsame as the documents. So basically, the way\\nwe had this was, we would assume we had 10- >> There\\'s also\\ndifferent lengths. >> Yes. We made some simplistic\\nassumptions of saying, there are 10 query words, 1,000 documents, zero\\npadding if there is less, truncation if there is more, and then a window one by\\nthousand and moved it for us. >> And the window\\nis one by 1,000? >> One by 1,000.\\nSo, that\\'s query was being 10 and 1,000\\nbeing the document words. So, it\\'s the length\\nof the document and moving in the query direction. >> So then, you\\'re not capturing the thing that you\\'re talking about. Where\\ndo you get these lines? >> Yes. So, what happens here, that\\'s a good question, is it actually does get\\ncaptured to some extent. But again, this is just\\nhaving a single convolution. You can do different kinds of convolution and\\ncombine these things. But even in this case, it does get captured crudely\\nwhere what happens is, by having a vector\\nhere, it can still, if you can just hypothetically, this vector could\\ncapture that this term matches a lot in the beginning versus later in the document. So, this vector could\\nencode such an information. It wouldn\\'t encode\\nexactly if they are next to each other\\npotentially, but yes. >> If you do directly,\\nlike three by ND. >> You could do other convolutional\\nconfigurations in addition to having\\nsomething that captures the whole\\ndocument length. We just didn\\'t go\\ninto that space. But I\\'m pretty sure\\nyou can do a lot of different\\narchitectural stuff. >> And you said that\\nit was important to make it as long\\nas the documents, so that you don\\'t have\\nany shifts this way. >> Yes. So, that\\'s basically because we noticed\\nthat it actually learns that if you have the query terms up\\nhere in the document, it\\'s much more\\nimportant than if it appears later in the document. And that\\'s important enough- >> -in the next layer. >> It could. Well, we actually had run a few different experiments where\\nthis showed up. This is obviously,\\nthe explanation is a hypothesis that that\\'s\\nmaybe what is happening. But we did see that it made a big difference in\\nhaving this contingency. >> Have you thought\\nabout filling this up not just with co-occurrence. I mean, matching up\\nthe words but some sort of a measure similarity in some- >> Yes. There is other work. If I\\'m not mistaken, like the match 10 serve paper\\nfrom Facebook and others. They\\'ve tried\\nother architectures where you basically\\nhave the lexical and semantic and maybe you have the same kind of matrix representation\\nthat captures both, whether the terms\\nare exactly same or- >> You\\'re talking about\\nthree or four different things- >> Let me go ahead and see if we can do a\\nfew more of these. That stuff is available\\nall week and I think maybe a longer conversation with\\nNabosa might be in order. >> Yes. I thought so. Yeah. >> Are there\\nfew other questions? >> I have a question about local verses global model\\nyou were talking about. The way that I understand\\nit is that it\\'s a neural version of\\npseudo feedback, basically. So that you compare it\\nto traditional method of trying to do pseudo feedback. >> Yes. Actually Ahmed and, I think Ahmed and Bruce actually wrote\\na paper where they, basically, this wouldn\\'t\\nimprove over pseudo relevance. This would not perform better than pseudo\\nrelevance feedback. But they combined it with pseudo relevance feedback and showed that it improves on top. But that\\'s exactly correct. So, this paper was written in the context of\\nquery expansion. But the core idea that Fernando and I,\\nespecially Fernando, was really trying to put\\nforward is this whole point of learning a model specific\\nto the situation you are in. And so basically learning at runtime and going from there. But it was tested\\nin that context. >> So, I have a follow\\nup question related that I know I would\\ntalk to you later. When pseudo relevance\\nfeedback fails, it can fail badly. If you have\\na bad starting value, you\\'re going to do poorly. But it seems like if we\\nuse this in two places, which is why bother use\\nthe pseudo relevance feedback? We have query logs that\\nhave click information, which give you weights on which one should be the focus for\\nany query we\\'ve seen before, how to expand for that query, or even in a session based case, using the previous\\nclick documents in the session to expand\\nfor the next query, where we know the thing\\nstays topically constrained. Have you looked at or thought\\nabout either of these? >> Maybe I didn\\'t\\nget your question. >> The first one is, for\\nany query we\\'ve seen before, instead of looking at top 1,000, you can do a click\\nbased weighting, click base if you like, to\\nlook at those documents. That\\'s going to get me\\na much more targeted at relevance and not\\nhave the failures of pseudo relevance\\nfeedback so we can do expansion for any query\\nthat we\\'ve seen before. The second case is you can do same thing at\\na session based example, which is the documents that somebody has clicked\\nbefore current query, is there a case of what is\\nrelevant to this topic, right? And we know that people think topicly relevant\\nwithin a session. Again, you are in\\na much better case by not doing pseudo\\nrelevance feedback. >> So, two different answers. One is, in terms of\\nthe first part of that, I think this whole point\\nof local global analysis, if you think of the spectrum, the local analysis becomes\\nreally interesting when you\\'re dealing with a query that has effectively no presence\\nin your click data. So for example,\\nthe Pecarovich Land Company. You would expect that if there are even 50 documents\\nin your index, it might fetch\\nthose 50 documents and those 50 documents might\\nbe enough for you to learn a local representation and\\ndo something interesting. If you have enough presence\\nof that in click streams, you could imagine that\\nthe global representation might already\\ncapture a lot of it. Of course, there might also\\nbe a sweet spot in between where it has some coverage from click data but not enough that your global representation\\ndoes a good job. So maybe, you can use that. Yeah. You can\\ndefinitely use that. The second part about looking at previous documents\\nin the same session. Again, I\\'ve not looked at it in the context of document ranking. But one of the things we chatted briefly before that I was super interested but haven\\'t really made a\\nbreakthrough there is, when I was training\\nthe DSSM model on spares of queries\\nfrom session, the intuition that I really had was kind\\nof starting to think about such sessions as effectively a path in\\nan embedding space. And if that\\'s going\\nto allow us to do something interesting\\nin that direction. Right? And that was trained\\non queries but it could also be trained on queries and click documents and whatnot. Again, I haven\\'t done\\nanything on that. It\\'s something that\\'s been\\non the back of my head. If anybody\\'s interested to chat about that or has\\nany ideas about that, I would love to talk about it. >> Quick follow-up\\nto the follow-up. >> All right. Last question. >> The 2,000 documents you are using helped\\nyour ranking or? >> This was based on Indri. So, he was Fernando- >> -type data. >> Not in here. >> You wouldn\\'t use\\na full search engine, right? You would be incorporating- >> No. This was just based on retrieval and\\njust something simpler model. That\\'s an important\\nquestion, by the way, which I\\'m not\\ngoing to touch upon is how you evaluate\\nthese models. Especially the deep\\nneural models on top. There\\'s this whole\\ndiscussion about telescoping evaluation\\nversus other things. But I\\'m going to\\nrefrain from that. We can talk about it separately. But it\\'s actually\\nan interesting question. >> All right. Thanks\\nfor speaking again. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Сегментация на главы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww = int(len(text.split(' ')) / 30)\n",
    "ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tt = TextTilingTokenizer(demo_mode=True, w = ww)\n",
    "s, ss, d, b = tt.tokenize(text[:].replace(\"\\n\", \" \").replace(\".\", \".\\n\\n\"))\n",
    "ttt = TextTilingTokenizer(w = ww)\n",
    "tokens = ttt.tokenize(text.replace(\"\\n\", \" \").replace(\".\", \".\\n\\n\"))\n",
    "pylab.xlabel(\"Sentence Gap index\")\n",
    "pylab.ylabel(\"Gap Scores\")\n",
    "pylab.plot(range(len(s)), s, label=\"Gap Scores\")\n",
    "pylab.plot(range(len(ss)), ss, label=\"Smoothed Gap scores\")\n",
    "pylab.plot(range(len(d)), d, label=\"Depth scores\")\n",
    "pylab.stem(range(len(b)), b)\n",
    "pylab.legend()\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Генерация ключевых слов для глав."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 different\\ differently\\ representation\\ representations\\ learning\\ query\\ neural models specifically\\ text\\ specific\\ search\\ big\\ recent\\ recently\\ online\\ retrieval\\ retrieve\\ today\\ office\\ main\\ possible\\ actually\\ traditional\\ framework\\ frameworks\\ model\\ right\\ popular\\ papers\\ nice paper\n",
      "1 actually\\ feature\\ featurize\\ featurization\\ features like\\ different\\ example\\ examples\\ seattle\\ representation\\ representations\\ words\\ word\\ start\\ starts\\ space\\ spaces\\ embeddings\\ embedding\\ items\\ item\\ based\\ topic\\ particular\\ seahawks\\ zero\\ thing\\ things\\ vector\\ vectors\\ similar\\ similarity\\ wilson\\ long\\ neural\\ people\\ explicitly\\ plus\\ come\\ coming\\ banana\n",
      "2 query\\ queries\\ different\\ documents\\ right\\ modelled\\ models\\ like document\\ suffixes\\ suffix\\ embedding\\ actually\\ actual\\ similar\\ similarity\\ rank\\ ranking\\ london\\ prefix\\ prefixes\\ text\\ model trained\\ big\\ tasks\\ task\\ search\\ expects\\ expect\\ cross\\ signal\\ start\\ question\\ examples\\ example\\ pairs\\ pair\\ train\\ training\\ chicago\\ san\\ seattle\n",
      "3 like\\ likely\\ word\\ words\\ training\\ train\\ query\\ queries\\ trained basically\\ actual\\ embeddings\\ embedding\\ model\\ models\\ completely\\ completion\\ similarity\\ similar\\ actually use\\ things\\ thing\\ documents\\ document\\ different\\ difference\\ term\\ terms\\ termed\\ context\\ away\\ learn\\ learning\\ learned\\ learns\\ somebody\\ started\\ starts\\ start\\ useful\\ example\\ text\\ anyways\\ run\\ running\\ big\\ prematurely\\ wikipedia\\ nyu\\ bit\\ representation\\ representations\\ vector\\ yes\\ right\\ topical\\ based\\ goes\\ window\\ size\\ inaudible\\ chicago\\ long\n",
      "4 model\\ modeling\\ models\\ like\\ likely\\ queries\\ query\\ word\\ actually\\ actual\\ documents\\ differently\\ difference\\ differ\\ terms\\ term\\ learn\\ learning\\ words trained\\ right\\ different way\\ document ranking\\ data\\ datas\\ based\\ local\\ locally\\ matching\\ matches\\ training\\ train\\ global\\ representation globally\\ representations\\ thing\\ things\\ having\\ yes\\ text\\ topic\\ topics\\ basically\\ think\\ thinking\\ duet\\ specifically\\ specific\\ let\\ sample\\ samples\\ embeddings\\ embedding\\ dealing\\ deal\\ key\\ mismatch\\ matrix\\ lines\\ line\\ textural benching\\ slightly\\ cut\\ cutting\\ worked\\ works\\ work\\ rank\\ simple\\ faster\\ looking\\ looks\\ look\\ looked\\ meanings\\ mean\\ means\\ real\\ pekarovic\\ saying\\ says\\ space\\ crude\n",
      "5 model\\ models\\ different\\ difference\\ query\\ documents\\ document\\ fields\\ field\\ like\\ text\\ matches\\ matching\\ match\\ matched\\ clicked queries\\ embedding\\ embeddings\\ actually\\ frequency\\ train\\ trained\\ training\\ basically learn\\ thing\\ things\\ right\\ score\\ scores\\ neural\\ learning\\ note\\ wise\\ paper\\ papers\\ precise\\ patterns\\ connected\\ click\\ multiple\\ cross entropy\\ similar\\ way\\ important\n",
      "6 different\\ difference\\ model\\ models\\ click\\ clicked\\ query\\ queries\\ actually\\ yes\\ documents\\ document\\ questions\\ question\\ based\\ base\\ feedback\\ like\\ neural\\ right\\ think\\ recommendations\\ global\\ local\\ versus\\ matches\\ matching\\ match\\ actively recommending\\ land\\ network\\ use\\ starting\\ exactly\\ convolution\\ convolutional\\ case\\ topically\\ topic\\ things\\ thing\\ learning\\ learned\\ learns\\ learn\\ relevance\\ relevant\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for token in tokens:\n",
    "    print(i, my_keywords.kw_extr(token.replace(\"\\n\", \" \"), 'gensim').replace(\"\\n\", \"\\ \"))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Сопоставление начал глав с временной шкалой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "ii = 0\n",
    "for j in range(1, len(data)):\n",
    "    for i in range(ii, len(tokens)):\n",
    "        if data[j]['text'].replace(\"\\n\", \" \")\\\n",
    "                .split('.')[0] in tokens[i].replace(\"\\n\", \"\")\\\n",
    "                .replace(\".\", \"\"):\n",
    "            timestamps += [data[j]['start']]\n",
    "            ii += 1\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_min = [str(int(x // 3600))+ ':' + \n",
    "                  str(int(x // 60 % 60)) + ':' + \\\n",
    "                  str(int(x % 60)) for x in timestamps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.231 0:0:7\n",
      "15.031 0:0:15\n",
      "1045.436 0:17:25\n",
      "1430.871 0:23:50\n",
      "1474.596 0:24:34\n",
      "1710.981 0:28:30\n",
      "2045.641 0:34:5\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(timestamps_min)):\n",
    "    print(timestamps[j], timestamps_min[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Финальный результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0:0:7 ['different\\\\', 'differently\\\\', 'representation\\\\']\n",
      "1 0:0:15 ['actually\\\\', 'feature\\\\', 'featurize\\\\']\n",
      "2 0:17:25 ['query\\\\', 'queries\\\\', 'different\\\\']\n",
      "3 0:23:50 ['like\\\\', 'likely\\\\', 'word\\\\']\n",
      "4 0:24:34 ['model\\\\', 'modeling\\\\', 'models\\\\']\n",
      "5 0:28:30 ['model\\\\', 'models\\\\', 'different\\\\']\n",
      "6 0:34:5 ['different\\\\', 'difference\\\\', 'model\\\\']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for j in range(min(len(tokens), len(timestamps_min))):\n",
    "#     print(token.replace(\"\\n\", \" \"))\n",
    "    print(i, timestamps_min[i],  my_keywords.kw_extr(tokens[j] .replace(\n",
    "                    \"\\n\", \" \"), 'gensim').replace(\"\\n\", \"\\ \").split(' ')[:3])\n",
    "    i += 1\n",
    "    #     print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

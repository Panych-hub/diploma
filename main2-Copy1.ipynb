{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from summarizer import Summarizer\n",
    "from nltk.tokenize import TextTilingTokenizer\n",
    "\n",
    "from matplotlib import pylab\n",
    "\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "import my_func\n",
    "import my_segment\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \">> It's my pleasure to\\nwelcome Bhaskar Mitra today.\",\n",
       "  'start': 3.9,\n",
       "  'duration': 3.32},\n",
       " {'text': 'Bhaskar is actually stationed', 'start': 7.231, 'duration': 2.579},\n",
       " {'text': 'in our London office currently.',\n",
       "  'start': 9.811,\n",
       "  'duration': 1.779}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = 'https://www.youtube.com/watch?v=g1Pgo5yTIKg'\n",
    "# link = 'https://www.youtube.com/watch?v=3vRC5TMbkaU'\n",
    "\n",
    "link_id = link.replace('https://www.youtube.com/watch?v=', '')\n",
    "\n",
    "data = YouTubeTranscriptApi.get_transcript(link_id)\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''.join([data[i]['text']+' ' for i in range(len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_main = my_func.my_processing(text)\n",
    "my_func.my_save_file('text_main.txt', text_main)\n",
    "text_without_punct = text_main.replace('.', '').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_NNsplit = my_segment.segment('NNsplit', text_without_punct)\n",
    "# my_func.my_save_file('NNsplit.txt', text_NNsplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_NNsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12146"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ww =  500\n",
    "tt = TextTilingTokenizer(demo_mode=True, w = ww)\n",
    "s, ss, d, b = tt.tokenize(text[:].replace(\"\\n\", \" \").replace(\".\", \".\\n\\n\"))\n",
    "ttt = TextTilingTokenizer(w = ww)\n",
    "tokens = ttt.tokenize(text.replace(\"\\n\", \" \").replace(\".\", \".\\n\\n\"))\n",
    "pylab.xlabel(\"Sentence Gap index\")\n",
    "pylab.ylabel(\"Gap Scores\")\n",
    "pylab.plot(range(len(s)), s, label=\"Gap Scores\")\n",
    "pylab.plot(range(len(ss)), ss, label=\"Smoothed Gap scores\")\n",
    "pylab.plot(range(len(d)), d, label=\"Depth scores\")\n",
    "pylab.stem(range(len(b)), b)\n",
    "pylab.legend()\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tt = TextTilingTokenizer(demo_mode=True)\n",
    "\n",
    "# s, ss, d, b = tt.tokenize(text[:].replace(\"\\n\", \" \").replace(\".\", \".\\n\\n\"))\n",
    "# ttt = TextTilingTokenizer()\n",
    "# tokens = ttt.tokenize(text.replace(\"\\n\", \" \").replace(\".\", \".\\n\\n\"))\n",
    "# pylab.xlabel(\"Sentence Gap index\")\n",
    "# pylab.ylabel(\"Gap Scores\")\n",
    "# pylab.plot(range(len(s)), s, label=\"Gap Scores\")\n",
    "# pylab.plot(range(len(ss)), ss, label=\"Smoothed Gap scores\")\n",
    "# pylab.plot(range(len(d)), d, label=\"Depth scores\")\n",
    "# pylab.stem(range(len(b)), b)\n",
    "# pylab.legend()\n",
    "# pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its my pleasure to welcome bhaskar mitra today. bhaskar is actually stationed in our london office. currently hes been at microsoft now about 11 years is that right yes about 11 years as a scientist in bing and somewhere. during that time. he switched to being a student about a year or two ago still while working at bing so hes working full time and completing his p at ucl under emine yilmaz who many of us know and have collaborated with as well. hes become one of the leaders in applying neural models specifically to information retrieval problems. he and nick craswell have a nice paper if you would like to read it. thats available on the web right now currently under submission to foundations and trends and information. retrieval that compare traditional ways of approaching information. retrieval models and where their parallels are in the neural world and hes going to talk to us about. some of his recent work so with that ill have bhaskar take it away. thanks for the nice introduction so todays talk i m going to be focusing on some of my research that ive been doing for the last couple of years. both in the context of my work in bing as well as as paul mentioned. ive recently become a year and a half back a research student at ucl so the research in that context as well. this is not going to be a general overview of neural ir but as paul already also plugged in nick and i have been working on a manuscript on an overview of the new neural ir field in recent years. so this is currently under review for foundations and trends in information retrieval. but we put up a early pre print online nick and i and also some folks from university of amsterdam. weve been doing a few tutorials recently this year. early specifically wisdm and sigir. so if you are interested again on a broad overview of this area theres also all the slides and materials from those tutorials that are available online so having said all that now coming back to the stuff that we are actually going to talk about today neural networks. weve seen a big penetration of neural networks. in many different application areas. the same is true for ir if youve been keeping track. for example at sigir papers. we have seen a pretty big spike in terms of the papers that focus on neural models in last couple of years and this also happens to be sort of my. personal main focus for my p as well as work for bing. the part that im really interested in when i talk about neural networks is that if you look at how neural networks has entered the different fields. whats also has happened is if you look at speech or vision and all of these other application areas. each of these application areas has different requirements and challenges and they have led to different kind of understanding or innovations in the field of machine learning. so thats the area that i really want to get into where were thinking about doing machine learning differently based on being motivated by the specific challenges in ir so what do. i mean by ir what would a typical ir task look like so. the most popular ir task that we probably all familiar with is a web search right so it involves. you have a short text. query. you have an index of billions of long text documents. and then the search engine fetches you top key documents and presents it to the user. but when i m talking about information retrieval today. it also includes other ir tasks. so for example i would refer to query auto completion where given a prefix you are trying to help the user formulate. the query by completing the query or recommending possible suffixes for the prefix. ill also give some examples of ir task where ir task could be like. predicting. the next query in a search session. so ill take these three tasks as example for some of the discussions that well have during this talk. one of the ways i can fit. any of this ir task is in this kind of a crude framework. so this is very intentionally a crude frameworks so bear with me. when i describe it this way but basically you can think of all ir task as you have some input text which could be a query. it could be a query prefix or something else. and then youll have a set of different candidates that you want to retrieve from or rank within. so for every candidate and an input pair and you can have some representation of the input and the candidate or potentially a joint representation and then from that representation. you estimate relevance. the reason i bring this framework up is because while we were writing that paper for fntir. this helped us classify how different neural approaches have tried either influencing learning query representations or learning representations of documents or even just applying neural networks just for estimating the relevance. so for example the very popular learning to rank framework. the way that would fit into this visualization is that you have an input query text. you have a document text and then youre generating manually designed features from these two to give a joint representation and then you can have a neural network or any other machine learning model of your choice for estimating relevance. the part that i am primarily interested in is actually the representation learning part and thats what i m going to be focusing on during this talk so but before. i talk about specifically neural models for ir. i wanted to make a slight segway to talk about something that personally is a favorite topic of mine to just talk about the basics of different kind of vector representations and specifically this topic of the notions of similarity in different representations.\n",
      "\n",
      "\n",
      " so this slide is really really kind of the fundamental. so everybody here knows it so just bear with me as i quickly go through them. so just defining a couple of things. here local representations and distributed representations so what do i mean by them. so these are terms that are pretty popular especially in the neural network literature. so a local representation or one hand representation is when you have let s say you want to present k different items. then you represent each of these items by a vector of length k where all the positions are zero except for a particular position that corresponds to that item. so let s say in this particular example if i had a set of different items of which three items were banana mango and dog. then in my one hand representation or local representation. for banana you have everything as zero and then one position that has a one or a non zero value on the other side. if you talk about distributed representation. this is the idea of distributed representation. is value represent items by a vector while a particular item is represented by having more than one non zero items in your vector. a very simple example of this would be if you think of any kind of feature space so imagine. i had a feature space. where i had features like does. this item bark is it. a fruit is it of a particular shape. does it have a tail and so on so forth. so if you imagine some kind of a feature space like that then you could say that a dog could be represented by something that barks and has a tail. but as a banana. it could be represented by something thats a fruit and has a particular shape. the interesting thing about distributed representation as opposed to local representation is that the moment you have this kind of a feature representation. you can actually reason about which items are similar to each other based on the features. so for example in this space you can see that banana and fruit are similar because theyre both fruits banana and mango and they are different from dog because they dont have any overlapping features but as if youre talking about the local representation. all of these items are distinct because no vector is similar to another vector in that space. the important corollary here to point out is that the moment you define a feature space you are in fact implicitly or explicitly making a choice of what items are similar to each other. for example you could easily come up with the feature space where let s say a banana and a dog might actually be more similar to each other than a banana and a mango. so maybe your feature space is about. i do nt know some kind of shape like is it long versus is it round or something on those lines or some something based on color by which you could actually say that in that feature space banana and dog is more similar than let s say mango. so my last definition on these slides is about embeddings so youll hear a lot of like especially in the neural network. all of the people say embedding is a very popular topic. one of the things that i just want to highlight is that an embedding is a new latent space that retains the properties and relationships between items from an original feature space. so a lot of the work where we talk about let s say inaudible or other kind of feature. other kind of embeddings that we talked about. one thing to keep in mind is that all of them are derived from an original potentially sparse feature space and the embeddings basically have the same relationship as in the original feature space. however your embeddings might be because of the nature of them being more smaller dimensional and dense. they might be generalizing more or other factors. but the important thing is that a lot of the relationships that are interesting properties that youre seeing in your embedding space are actually existed in your original sparse feature space as well. why is this important. this is important because. i personally have found that its quite difficult to reason about embedding spaces because of the nature of these dense vectors. but if you think about the same items and their original sparse feature space. sometimes you can build a lot of other kind of intuitions that can be used. well. ill give some examples of the same thing right so before. i go into the example. this is the kind of the question that i wanted to put forward and this is one of my favorite topic so bear with me. if i talk about this a bit too long so. this is the idea of thinking about explicitly thinking about what kind of relationships are you capturing in a certain vector representation. so let s say youve learned an embedding of words or concepts. and then you have seattle and the question might arise is seattle similar to sydney because theyre both cities or should seattle be similar to a seahawks because of seattle seahawks. so i want to take some example and actually answer. this question about like how some of the choices you make in designing or how you learn your embeddings would actually influence which of these relationships you would actually modeling and your embedding space actually captures. so. what im going to do. here is im going to take a really small toy corpus of 16 short sentences and from those 16 short sentences i m going to try and derive different kind of representation for the four domes. seattle denver broncos and seahawks and basically my goal. here is to show that depending on how you featurize these words. based on this corpus you actually end up capturing different kind of relationships between these four domes. so the first thing i m going to do is i m going to featurize these words based on which documents they appeared in. so we see that in our document for example seattle appears in document. one and two seattle seahawks appears in document three. they both appear in document five and so on so forth just a quick note on the colors here. so the white here means a zero. the grey means a non zero value and the only reason i have a light and a dark gray is to indicate those columns where you have more than one non zero element. so the diagram basically shows there is some overlap between at least two of the vectors in here. so basically if youre going to squint at this particular diagram hard enough youd start seeing that the vector for seattle based on this very crude rudimentary example actually starts looking very similar to the vector for seahawks and similarly if you look at denver and broncos. these start looking similar to each other for people in ir. this immediately would remind you of the dome. document metrics that you would typically factorize for lda and other representations right so now let s do a jump and do a very different kind of featurization based on neighboring words. but with one small additional detail that im actually going to consider the distance between the two words. so what do. i mean by that what i mean by. that is that i have a feature. seahawks plus one which means that this is a feature that says that on one position to the right the word seahawks appears so for seattle seahawks. you have a non zero value because somewhere in here you have a bunch of seattle seahawks that you see in your corpus. so then you also have a lets take. the example. seattle seahawks wilson so the word wilson actually ends up giving you two different features. wilson plus one and plus two. so wilson plus one means it occurs. one position to the right of this word which is true for seahawks. so you will see that wilson plus one actually is a feature for seahawks but not for seattle. but wilson plus two which means its two position. to the right is a feature thats non zero for seattle but again its zero for seahawks so you take this feature space and now again do the same exercise of squinting really harder. this diagram and suddenly you start seeing that seattle and denver starts looking. similar and seahawks and broncos starts looking similar right so this starts giving you more of a notion of a tie. based similarity in this diagram. by the way for people in nop. people refer to this for a pretty long time as kind of syntagmatic and paradigmatic relationships on using a different dome. but i mean something very similar. the third version is similar to the previous one. but we do want change where we no longer consider the distance between the words. so now. wilson is a single feature. thats true for both seahawks and seattle and again. if you do the squinting thing again. what would happen here is now. we actually see that pretty. much. all four of them has some kind of an overlap and like seattle is similar to both seahawks and to denver but because of different sets of features if you think about it. this is exactly the kind of feature space that something like word2vec or glove operates on. so the point that im trying to drive towards is that if you think about this feature space you realize that its the property of which words are similar to each other  or the fact that something can do. a vector algebra in the embedding space is actually coming from this original features space so this for example levien. goldberg did this work where they showed that the vector algebra like in minus man plus woman. you could actually do this on the sparse feature space without even and learning the embedding stuff and similarly it doesnt really matter if youre using metrics factorization on neural nets or some other approach. the relationships. youre modeling is actually much more influenced by the basically what original feature space youre. trying to compress this by the way. one of the things about word2vec is the window size that you consider when training. word2vec actually has some strong influence on the balance between the type and topic based similarity. so basically if you think of this. this is the reason why in the word2vec embeddings you would see seattle being close to both seahawks and denver. so it has a mix of this type and topic similarity and how much of this type versus topic might depend on the hyperparameters for example like the window size and stuff and which im not going to go into details. but we can talk about why that it is after the talk. i wanted to jump into this example because. this often is pretty intuitive when you look at this visually like this. but then obviously the questions come up. why is this important so why are we talking about this in the context of neural ir this is important especially because so if you think about the ir problem to be honest. this is true. for any variable. you use embeddings not just in ir but taking the example of an ir task. what you would typically do is. given. let s say an input text like a query or a prefix. you could come up with some projection into an embedding space.\n",
      "\n",
      "\n",
      " and then your assumption is that the neighbors in that embedding space are the good results or candidates that you want to rank high and show to the user. so what that also means is that for a particular input. if these are all my right candidates. then it means that these right candidates should also be similar to each other. now if youre talking about different ir tasks. so we talked about in the beginning like document ranking. query auto completion or related search. the argument that im really trying to make is that each of these different ir tasks expects different notions of similarity between the different items that theyre trying to rank and its important to understand what relationship youve modelled in your feature space and whether or not thats appropriate for the task that you are trying to apply to so going back to the same. three examples so let s achieve my document ranking example as that the query is cheap flights to london and let s assume these are the titles of the documents that i m ranking so. cheap flights to london obviously should be relevant to the document. cheap flights to london. but then obviously you dont want to see documents about cheap flights to sydney or hotels in london right. however if you take the query auto completion task imagine. this is also modelled in a similar way via given a prefix. i have some prefix embedding that projects into a space and then i m doing nearest neighbour search based on suffixes. so i m just ranking suffix. let s assume thats how we do the query auto completion task. here. so here now. what happens is if you have the prefix cheap flights to you actually would as you expect london and sydney to be there. they are potentially right answers. here. so what that means is that in this space if youve learned the representation. london and sydney should actually be close to each other whereas they definitely should not be close to each other. here on the other hand. london and big ben should not be close to each other here. whereas here you could potentially rank a document high that also mentioned big ben. so this very kind of a topical similarity versus type similarity and then well talk of next query suggestion theres. a whole lot of other things that opens up. so for example. here. the same topic is obviously relevant but youre also trying to predict the next step in the task. so here theres almost like a directionality factor as well. like. you can think of certain pairs of queries where query b will always follow query a and not the other way around. so for example if you see two queries like the big clock tower in london and big ben. you would probably expect that the big clock tower in london query came before big ben because it indicates that the user did not know what its called so theres. a potentially different kind of relationship that you would need for doing this particular task i m going to stay on this topic a little longer by showing another single model trained on three different types of data that demonstrates these three different types of relationships. so this is the very popular. the dssm model so again. people here should be pretty familiar. with this model. the dssm model is a siamese network. it was originally proposed for the document ranking task. what this model basically does is. it takes a query. it takes a document title. it projects the query into some embedding space of 120 dimensional embedding space. it projects the document title in the same space and if the query and document are relevant to each other. then you would expect the distance to be small. the cosine similarity to be high and the way you train. this model is. you. take a query a positive or a relevant document for that query and you take a bunch of negative documents which are not relevant to the same query and you train to optimize cross entropy at the top to predict the right document from the negative documents. so now we take the same exact model. the model doesnt really specify what text like you could present any pairs of short text here and train. this model so let s take the same model and actually train it on three different types of data so were going to train this on query document titles which was originally proposed. we are also going to train it on prefix suffix pairs so well take a query split. the query randomly at a word boundary and then train the model on given a prefix predict the suffix and then the third one is to actually take pairs of query from such sessions and then given a query you try to predict the next query in the same session and basically what you end up. seeing is that if youve trained the query let s take a particular example. let s take the two models trained on query document pair and prefix suffix pairs. once youve finished training the model you take the query model from the first set. you project a piece of text in the space. and then you look at its nearest neighbor and on the other side you take the query that was trained on prefixes and you do the exact same thing youre. given a piece of text. you project it using this model. and then you look at its nearest neighbors. you almost immediately see this pattern especially by the way the models are trained on short text sorry. this was not for short text. ignore. the last comment so what you immediately start seeing is that the model thats trained on query document pair. the nearest neighbor to seattle is weather seattle. seattle weather ikea seattle everything about seattle whereas if you look at the model thats trained on prefix suffix pairs. its nearest neighbors are all these other cities. chicago san antonio and denver and so on so forth. similarly if you project query on the piece of text. taylor swift. the model thats trained on query document pairs gives you everything about taylor swift. whereas the model thats trained on prefix suffix pairs. it actually gives you. everything. thats of the same type as taylor swift like celebrities. kind of the very same idea as we were looking at before inaudible. sure how do you train the prefix suffix pairs. so you basically take query compass. for every query. you randomly split at a word boundary. and then you would pass the prefix here and the suffix here and some negative suffixes here. so the actual paper that this one cites. this is where we were looking at suffix. prediction. given prefix for dealing with auto completion for rare prefixes where you actually havent seen any query with this prefix. if you still had a candidate of let s. say 100000 most popular suffixes. then you might still be able to predict with suffix goes with the prefix. do. you have two different endpoints on the left and right yes so the siamese network. whats happening is its. actually. you have two different models. so the weights are not shared between this and this. but this is the same model here right okay that makes sense one question. yeah. if you go back to the previous slide it seems like. there are two things going on from what you said. earlier. one is the supervision signal of whats related and then. the other question is the co occurrence of different approaches that people often apply if you take the embedding so take. some general embedding embed these things and now use this revision signal to actually learn for each of these domains so that you dont complete. both of them go back to the earlier example. if the relationships were there in embedding. then one thing you can do is just project to the right relationship and there is a separate question of are the relationships even there in the first place. now youre getting how much did you get from each of those. the other thing is also that you might have learned an embedding with certain relationships thats almost so orthogonal to the task that you are trying to learn. so one example is in bing when we were working on. autosuggest. somebody took a particular embeddings that was trained basically to capture topical similarity. sort of trained on like basically query and documents and then what they wanted to do is. they wanted to actually use that for auto completion so what they did was. they initialized the embeddings based on what was pre trained and then they did continue training and they actually realized that it was worse than randomly initializing the embeddings because what happened is if you think about it. if seattle is close to seahawks and far away from sydney. but in the query auto completion where somebody says map off where seattle and sydney should be close together. this embedding had to actually push these two things apart and pull those two things that were far away close together. now. if thats your only representation of these items and youre just trying to learn a function on top then it depends on whether the original embedding space has captured the useful information in addition to capturing other stuff. but it could also be that the original embedding space is completely the wrong space that theres no other function. you can apply on top to recover the actual useful information that youre looking for so it depends and basically. the argument i m trying to make. i was going to have that slide after this is that all of this really matters if youre going to learn representation separately and then use it for a different task which is actually pretty practical because a lot of time you dont have enough data for the supervision signal to do the learn representations. representation. learning takes a lot of data so thats what people for example will train what to work on the document corpus alone and then you might plug in those embeddings and doing ranking or something else so thats when its becomes important to think about is this even the right thing or if the thing doesnt work. then when youre. debugging then it comes to fact like maybe theres. a disconnect between the embeddings. ive learned and the tasks that im trying to apply it on or it gives you ideas about how to write learn the right relationships so that it might be more useful for the task so it doesnt matter. if youre learning everything and doing then this question goes away. the third example. i wanted to show was the case where the model was the same. dssm. model is now trained on pairs of queries from surf sessions and this again shows very different kind of regularity. so here you can actually see a similar kind of regularities that you might have. noticed. you can probably make a connection with like the word2vec at a term level. so this basically allows you to do similar kind of vector algebra on short text like you could do with word2vec on terms on because you see that denver to denver. broncos is parallel to san francisco. whoever the team is sorry. there you go seattle to seahawks. i should always stick to seattle because. i know the answer and yeah so. this basically if you train it on session query pairs. you can basically then use the same model to do. things like university of washington minus seattle plus chicago gives you chicago state university. obviously. this like any other vector. algebra. based solution obviously is not always going to be correct so you also have like wrong answers sometime here.\n",
      "\n",
      "\n",
      " but it just kind of shows you that it has a similar property and if you think about it. these models of training on pairs of query in a session or training on prefix suffix that actually similar to word2vec but its training on neighbors. whereas the original model if you think about training from query to document is more like training like an lda model where its termed to document id so. its kind of has those the same intuitions that pops up so yeah. so. i skipped. i set prematurely talked about the site. but this is the main point that you really care about all of this. if you are going to use pre trained embeddings. if your model just learns here embeddings in c2. for the task that youre doing. then you can be completely ignorant and let the model learn whatever is the right representation. so i want to give an example quickly of some. but this could be useful. this was actually a joint work with eric nalisnick. he was an intern here and rich caruana and nick craswell. so this basically led us to thinking about using word embeddings for document ranking. so in document ranking. one of the things that you could use. these embedding for is sort of soft matching. so for example let say we have these two passages and we want to know which of these two passage is more relevant to the query albuquerque. so theres a lot of hints. if you look at the first passage from the fact that it has words like metropolitan population area and so on and so forth. so if you knew at some way you could determine the other words you expect in a passage or a document that correlates with your query terms and that can be a useful signal for relevance. unlike typical ir models where you only count the exact occurrence query terms in the document and you can say that the second passage is not relevant because this passage is actually taken from wikipedia described from the wikipedia page on microsoft that just happens to mention albuquerque in there. so you take this idea so what you can then do is. you could given a query in a document. you could represent all the query terms using the word embeddings that you have. you can represent. all the document terms with the embedding. you have let s assume you just do a simple centroid of the query term centroid of the document terms and you have a simple cosine similarity or a dot product to say. if the query and document is relevant. let say thats the simplest model you can do now the interesting and this is where thinking about these relationship explicitly might help. you is so when we started. thinking about this. one of the things we suddenly realized is that if you look at the word2vec model theres actually two different embeddings that are being learned so the work2vec model. what happens is you have an input word represented as a one hand vector multiplied by a metrics multiplied by another metrics. this is the bottleneck clear and then youre trying to predict the correct neighboring words. what happens here is that you actually have two different weight matrices and i m going for the ease of reference i m just going to call them the in metrics and the out metrics so the in embeddings and the out embeddings. typically once a word2vec model is trained. people completely discard one of these matrices and just use the other one as the embedding. now if you train. your word2vec model on short text for example like queries turns out looking a similarity between two different terms based on both of their in embeddings or based on both of their out. embeddings actually gives you a notion of similarity that is closer to a type. a similarity. so yale becomes close to harvard nyu cornell and so on and so forth. however if you were to represent one of the terms with in the other terms with the out embedding and then do the dot product or cosine similarity you end up. the neighbors of yale starts becoming faculty alumni that is misspelled anyways orientation and graduate. it took me two years to realize that is misspelled thats something anyways for document ranking. what youre really looking for is if you have the query yale youre looking for a document that not only has the word yale in it but likely also has other words like faculty alumni and orientation and so forth. at least you expect that more than occurrences of harvard and nyu. although harvard and nyus presence might also indicate some kind of relevance. so basically. what we did in this case is we plugged in this intuition into the word embedding model we showed before. by basically doing something very simple. we would use the in embeddings for the query terms and the out embeddings for the document terms so that when you do the cosine similarity or the dot product between the query and the document youre. basically computing an in out similarity for every query term with every document term and that gives you an improvement in relevance. in this context a word and the output is the next word yes. so the word2vec is typically trained. given. a word predict another word within a given context window including itself or not including itself not including itself. its basically let s say. if the window size is 10 words on each side. it will try to predict each of the 10 words on either side of this particular word. how does that compare to using the dssm sector you mean in performance wise. yeah well. the desm would perform better just because its trained specifically for that task. i see right and the desm model is learning a topical notion of similarity because desm flips like youre trying to map that query to the document pair so you are extending the query in that sense. yes. i should clarify. i mean the desm model trained on query document pairs. if you look at the desm model trained on prefix. suffix pairs its basically learning a similar relationship as word2vec training on word and neighboring words. again you can use even that model. we havent really worked on this. but there is no reason why you cant actually train. if you had. for example small amount of query document pair of data. you can actually train the desm model on prefix suffix pairs and use it in the same in out way as the desm thing proposes for document ranking as well and in that case youre training on query yes okay. this model was trained on queries. i see basically. the two secret things. here is that if you train this model on long text and have a window the window of text long enough. then even the in in and out out starts becoming more and more topical. the balance moves towards it. so basically the reason this becomes important is you actually get a big benefit by training on query terms as opposed to document text and for short text the in in and in out similarity. actually the difference becomes bigger so you combine the two things and then you see a big improvement. but if you were to train this model on as. i said long text with long context window then you might see a less difference on this. so if you built this amounts that give you score of the next word and the previous word what happens if you create like ten slots in the sentence and then run proliferation to fill in the transport can you generate sentences that way. the word2vec was originally proposed as a simplification for rns if your goal is only to learn on embeddings. but this basically goes back to the same idea of sequence completion right and depending on a few different things. so for example you could train word embeddings where the word appearing next to you is different from the same word. appearing two position away from you. so you could actually take that into effect and again that would change the relationships youre learning but in terms of sequence completion yeah. this basically goes back to the rnn and all of those kind of word directly. inaudible. rns but just using this thing to generate sentences by simply running a little bit. variation through the slots you could you just have to in. that case also take into context. then the position and the distance. but there is yeah you could come up with a. i mean again. if you start doing that slowly it will start evolving into something that starts looking like rnn. basically. the point with rnn is that i want context of not just the previous or the last three or four words. i want to have longer and longer context. the whole rnn analyst theme is about how can you have some notion of the things youve already seen in the sequence with the fixed memory size. okay inaudible to understand what rnn is i m. just curious. what will happen. just in any case as opposed to rnn like. you can always have a model. that has if you knew your sequences are always short. you could come up with a model that always knows that there are at max key items before and you have an exact memory of what youve seen in the past and then youre not doing reckless. but you actually have different way matrices based on positions for the lobby. ive never worked in that space. so i wouldnt comment on what works or what doesnt but i m just saying the answer sorry and you do nt know. if anybody has tried that just in common i m pretty sure the nlb space people have but again. i would probably defer the question to somebody who operate in that area so moving on further and a bit faster think about it. what was the data thats available. we actually put out the in out embeddings data for two point. seven million words trained on bing queries. just publicly we had released. it is anyone using that training. i got questions from different people so i assume some people are but yeah. this was another work joined well with fernando and nick so basically in the same spirit of looking at word embeddings. the other question that came up is that obviously terms are inherently ambiguous. this is one of fernandos favorite topic and hes really convinced me that this is a very important thing to think about. hes got to think about this idea of local and global analysis right so the point is basically that let s say you learn a representation globally based on your whole corpus. so then youre basically learning some sort of a crude representation for every term. especially when the term is ambiguous and can take different meanings right and this is not just like the crude apple computer versus apple the fruit its also very subtle. for example the word cut could mean different things but then if i know that i m using the word in the context of gasoline tax. then this cut means like cutting tax or reducing deficit or whatever basically the idea was like. how can you get really extreme in thinking about a local sense of a term or a representation of a term. what we did is. we tried. this experiment in the area of query expansion. for retrieval. what we would do is given a query. we would send that query to the index and get a bunch of results. and then we would at run time train a work to work model on that corpus of top thousand documents and then use that embedding to expand the original query send it back to the corpus and get results and basically the point comes down to is this. what this diagram shows is. the blue dot in the center is the query centroid and the way you are expanding. the query is by finding other terms that are similar to the query center so basically youre doing a nearest neighbor around the space. these are just contour lines but every other circle here are. the terms candidate terms that you can use to expand. the query and the red terms are the actual good terms that we know that. if we add this to the query you would see an increase in ndcg and thats what it looks like for a global word vector space and this is what it looks like with a local vector space. but the idea here mainly is that. i think this actually relates to a slightly broader idea that for any kind of modeling for example when youre learning a query document ranking model all these neural net models are basically starting to encode real world information about correlations right. so if somebody says let s say albuquerque right so you expect that for the word albuquerque you would expect these things like altitude or population or area as to be the correlated words that you would expect in the document. but these models are always going to be inferior to a very very narrow topic like something specifically in microbiology. its very hard that this model can memorize those correlations and the idea was that we can think of actually training models potentially at run time or doing things differently such that these models can still go in and effectively research. a particular topic learn a new representation and then apply it to a problem. the metaphor that i use with nick is thinking about whether your model is a librarian or a library can. your model practically learn everything about everything in the whole universe and be a library or is it like a smart librarian who might not know enough but knows given a question on microbiology that thats the section in microbiology let s go there. let s read up all. the books learn a new representation. now. i know which book to look at and so on so forth so that was kind of the idea. but this was a crude way of doing this. the local embedding is trained on different area. the local embedding is given a query. it actually retrieves a top thousand documents and the embedding is trained on those thousand documents. the local one has not been trained from that data. it has been trained on a random sample of documents from the same compass but not for the query. specific inaudible seem the same there. yeah so this is like because usually when they talk about this bearing theyre. saying well all the information is there. you just have to just find the right projection. yes but the whole. in this case its just its actually different data. so you may not even be able to find the projection. so the point is even if your corpus is fixed its almost impossible for you to learn on everything in the corpus within so. this also goes back to thinking about machine learn model having certain amount of capacity and its going to prioritize. learning. things well that it sees more often and not learn those things that it sees rarely. whereas if you knew. all you wanted to do was to answer this question about microbiology. you could go find only those thousand documents on microbiology. so theres enough data. but the model basically doesnt prioritize if it has to learn the whole universe. but then it can go and learn that small universe much more efficiently and be more effective. this is hard to tell because its a very high dimensional embedding so you have space for everything. technically yes the difference may be in a very inaudible. it could be so there might be some thing that you could do to force. this. there maybe others solution to this but yeah. so let me come back to the question. in different data. i mean the local is just a sub sample of the global right. so with that different data its a subsample. no its different its actually its a different sample. its not different data. its not different documents its a subsample of a subsample of. but they are different samples. there different samples. they are not different. data right well. if everything was learnable from the global. then you would still have the relationships that global has access to everything local. if the global is big enough to get the samples if in general all the data. local is a subset of global and what if you sample. if you sample you will not get anything thats in the datas. you can go global over everything. it trends everything. but it is sub sampling from the overall distribution because so initialize the seeds and you can do a sub. sample relationship. same thing is done on the learning curve. basically the idea is that global might have a lot of noise. so some of the sample relationship may be drowned out by noise so and then kind of buy like two level ranking where you use some sort of naive textural benching type of stuff to discover lightly relevant subset and then you hope to discover locally. stronger. relationship from that subset is that kind of reasonable way to look at it. right so basically your global. the model its any model that you train on somebody thats going to prioritize learning things that it sees often. but then you can use. for example the global model to get some ranking of basically narrow down which domain you really. this belongs to and then you can have a local model thats trained on that domain specific data and then it would likely perform better on that specific topic. so if i was to put you said previously. for example the global corporates you will have many fewer documents on. like leverage. then youll sample these slots on purpose. so you dont see the same data. in the two cases. yes its true having said that. i would still imagine that if you did train on the exact same samples but like subsample. i think this would still hold. but what you described is what we write instead. if you just take the global data and then edit the local sample to it. now the question is those are two different questions. right would. you then have a problem with learning and still ignore the data just because but in this case you are not even seeing it so then potentially yes yeah okay so i m going to go into the last part of this talk where this is more recent work where we actually started. looking at deep. neural nets trained end to end so no longer thinking explicitly of these representations and all that stuff but having deep neural nets and kind of a very clean set up and training them for document ranking and obviously. this is all based on the fact that we have a lot of data in bing. so we can actually train. these models end to end on our ranking data so what we started. looking at was the kind of a classical document retrieval task where you have a short text. qwerty in a long text documents. a lot of the models in ir today actually looked at short text verses short text. the book and one of the things to kind of realize is the challenges of doing short text to long text are different both good and bad so short text to short text. this embedding based models actually benefit a lot from the fact that short text to short text. the vocab mismatch problem is much more severe. so if your model understands synonymy or related things its most likely to show you improvement over exact matching based similarities on the other hand. for long text. the opportunities lies from the fact that long text has mixture of many topics and the matches could be actually in different parts of the document and based on which part of the document matters. it might indicate more or less relevance and also things like term proximity and all of those other things becomes important. so theres a lot of possibility of modeling other stuff when youre dealing with long text not just the vocab mismatch. an interesting way of thinking about this neural nets for ir is to take these two particular queries. so these are my favorite poster child queries. pekarovic land company and what channel are the seahawks on today. so the point that i want to make with this query is pekarovic land company. if this is the term pekarovic youve never seen in your training data then. i dont care how you have your neural model input representation. so for example it could be 100 presentations of terms or character trigrams and whatnot your model will never going to have a good representation for the term pekarovic. so even for dssn that takes character trigram based inputs you would expect that for a term it has never seen before it would probably have some sort of a random projection or not. so very informative protection let s put it that way. so the point is that for this query for an embedding based model its actually it really struggles with a query like this right. on the other hand we know that typical a classic ir models actually really do well with this because if there are like 50 documents in your a billion index that has this term then fetch. those 50 documents rank them based on where this term appears and how they appear next to each other and so forth. but you could learn all of the same functions within neural net. on the other hand you have the query what channel are the seahawks on. today. here lexical matching model would actually suffer because the right document potentially doesnt even have the word channel in it. it probably has actually terms like espn or sky sports right so its almost like a translation model kind of thing happening here. if you see these words in the query you expect these other words in the document. so this is where an embedding this model is likely to really shine and one of the first work we did in the space was this thing. we call the duet architecture which was basically just the simple idea that hey why. not. if these are the two different aspects that might be important in ir why not use a neural net to learn both of them and learn them jointly so that basically led us to this idea of the duet model. now this is an architecture we proposed. but then the idea of the duet goes beyond just this architecture. you can come up with other architectures that has the same duet property that you explicitly are mapping. learning. the lexical matching as well as the semantic matching signal for retrieval and the final model in our case was basically a simple linear combination and youre. learning. this whole thing as a single model jointly on your supervised data. well. basically. it has query document and labels so training like typical learning to rank. models are based on pairwise loss so how do these things differ right so the local model its trying to learn how to do lexical match ing exact matching. it has no interest in learning any kinds of representation. so the input representation is such that it can not even learn any kind of embeddings in this case so whats the input representation look like so. these are really simple matrix representation so think about a query that has four words and a document that has thousand words. so the input representation for this sub model is a query words by document words matrix which is a binary matrix and its non zero every time the word matches the document. so this means that for example if you look at this line slightly shifting line it means that you actually saw the occurrence of big deal derby as a phrase in that document and if you look at these documents. here. these examples. its almost visually suddenly becomes clear which documents are relevant and which are not. so. the relevant documents has more matches. it has more matches towards the beginning of the document. it has more phrasal matches. it definitely has matches for all the words in the query. unlike this guy and so on so forth. so you can see these visual patterns from these matrix that then you can imagine that you can train a neural net on top to basically learn what are the good patterns of matches. so you are like ignoring total frequency completely. in this representation term frequency. no but inverse document frequency yes term frequency. you can get by actually just counting along this line right. so if the neural model can just count the number of lines in here and it knows the term frequency for each of these terms. so what we did is again. you can have different kind of architectures. but the simplest thing we did is. we had a convolutional model on top that one of the important thing was to have the window of the same length as the document basically inaudible. important thing is for it to map how early in the document you see the occurrence that was an important thing. but other than that. there we tried different things that didnt make too much difference but im sure. there are things here that you can explore but basically youre doing a convolution model that runs over that same matrix that i showed and then it has some more fully connected layers and so on. and so forth. then it gives you a single score in the line of the document. then there is no convolution. it moves along the query direction its moving along the query direction. so on the other side about the query and the action. yes theres. something that worked on proximity just because of time. i wanted too may be go faster and then we can come back to the questions but i m skipping some things about proximity and the query sorry so now on the distributors model what it does so. this again if youre familiar with the dssm work. this model is similar to the dssm with some changes just to specifically deal with long text. so what this model does is. it takes the query and then projects the query into an embedding and then it takes a document. but instead of projecting the whole document in the embedding space. it actually takes a window over the document and predicts each window to an embedding space. compares the query embedding with the window embeddings and then it aggregates all the information on the top and gives you a single score right so this is a very simple way in describing this model. for more details. you can always look at the paper so yes so basically. we then trained it in a very similar way. as the dssm model. you would have a query of positive document and a bunch of negative documents. you train it to maximize cross entropy loss. we have a bunch of different findings. but one of the main things we found is that and this was kind of expected is that the part that really takes a lot of training data to learn is that it text representation so the local model or the lexical side of the model actually doesnt benefit too much from being thrown a lot of training data at it. whereas the part that focuses on learning embeddings is the part that actually so for our first duet paper we didnt even converge. we basically trained it for like 24 hours or 36 hours and kind of stopped at that point. but basically if you look at the training curve it looks like it was still going up. so we could have had a lot more better mbcgs by training it longer and the only reason we did nt is because of time. we later ran the same model on trec complex answer retrieval task. but the data was smaller and my training time was faster and there we actually got to a point where it kind of flattened off at something around like 32 million samples and so on. and this is one of the important points why if you look at a lot of papers in academia. today its kind of unfortunate but a lot of focus is on lexicon matching model just because of the lack of big datasets for training these supervised models i m going to skip this slide. this is also out there. the implementation is out their publicly on github. but this let us then this is the kind of the last recent work. i wanted to make a mention of so. we had another intern ahmed who came in from umass earlier this year and he looked at again deep. neural models. given we had a lot of data. it was something he was interested in to again look at deep neural models for ir and the problem we looked at is the fact that its not just documents and web searches not just body text. but its also title and url and anchor text and clickstreams and all the other kind of data sets that we have right so. this work was basically kind of the same kind of you can compare it with like going from bm25 to bm25f. so basically how can you use a neural model that when you think of document as a single piece of text but my normal document is composed of multiple fields so how do you have a neural model that actually can make use of multiple fields. so this is a wisdom paper 2018. so basically the idea is this you have a document that has title text. url body incoming anchor text and incoming query text and what youre doing here i m going to just give you a quick overview so basically i m going to touching up on the main design decisions that makes a big difference. so you learn an embedding for each of the metastreams separately. because each of these fields of metastreams might have different properties so you want to learn different embedding spaces for them for fields that have multiple instances. so for example for anchor text and query text you have multiple clicked queries associated with this document and multiple anchored text. associated with this document. you are effectively doing an average pulling. but making sure you take care of how many instances are there and do the right. averaging. so by doing all of that you end up with these field embeddings and then you learn different query embeddings to match with each of these different fields and this comes from the fact that for example the query embedding you want to learn to match with title might be different from the query embedding that you want to learn to match with url because they captured different properties. once youve done this hadamard or element wise product. between these two embeddings you end up with a match vector corresponding to each of the fields. so this comes from the intuition from like same similar intuition as bm25f where you dont want to combine the different fields based on individual scores. so this is the same idea that you could end of let s say the query was barack. obama facebook by having a vector and not a single score. it allows the model to realize that maybe barack. obama matched on every field. but facebook never matched on any of the field versus barack. obama matched on this field and facebook matched on the url field so by having a vector it can encode that information. but it wouldnt if you had a single score there and then you have some fully connected layers on top to combine those and one of the things.\n",
      "\n",
      "\n",
      " thats important in this context is and this happens even for typical learning. to rank. model is that if you have a very strong precise signal such as clickstreams. it can actually hamper while you learn the other fields. so one of the things that we found really useful especially in the presence of a field like click data is to have a field level drop out in this model so yes so thats basically it. so basically. one of the things on a concluding note wanted to say was these deep. neural models obviously theres. a lot of excitement in the field and people have been thinking about like how this is goi ng to influence this ir field. in general. my personal observation is that all of the stuff that we typically talked of ir initially went into designing of features during the learning to rank frame days and now when were starting to talk about these deep neural nets. the same intuitions. scored intuitions from ir is actually fueling the design of these different architectures. just this one i m not going to talk about what weve also learned recently. some work on proactive recommendations such as pro actively recommending attachments for email and yes. these are all the papers that i covered and thank you sorry for going a bit over time. so we have time for questions for professor inaudible. those commercial network filters what they looked like. i did nt quite understand how you built that up. but it seems like it would be very interesting to see what these filters are. for the local model. the disclaimer is. we. did nt spend a lot of time doing too many experiments. but one of the things we did find was to have the length same as the documents. so basically the way we had this was we would assume. we had 10. theres also different lengths. yes we made some simplistic assumptions of saying there are 10 query words 1000 documents zero padding if there is less truncation if there is more and then a window one by thousand and moved it for us and the window is one by 1000 one by 1000 so thats query was being 10 and 1000 being the document words so its the length of the document and moving in the query direction. so then youre not capturing the thing that youre talking about where do you get these lines. yes so. what happens here. thats a good question is it actually does get captured to some extent. but again this is just having a single convolution. you can do different kinds of convolution and combine these things. but even in this case it does get captured crudely. where what happens is by having a vector. here it can still if you can just hypothetically. this vector could capture that this term matches a lot in the beginning versus later in the document. so this vector could encode such an information. it wouldnt encode exactly if they are next to each other potentially. but yes. if you do directly like three by nd. you could do other convolutional configurations. in addition to having something that captures the whole document length. we just did nt go into that space but i m pretty sure you can do a lot of different architectural stuff. and you said that it was important to make it as long as the documents so that you dont have any shifts this way. yes so thats basically because we noticed that it actually learns that if you have the query terms up here in the document its much more important than if it appears later in the document and thats important enough. in the next layer it could well. we actually had run a few different experiments where this showed up. this is obviously the explanation is a hypothesis that thats maybe what is happening. but we did see that it made a big difference in having this contingency have. you thought about filling this up not just with co occurrence. i mean matching up the words but some sort of a measure similarity. in some yes. there is other work if im not mistaken like the match 10 serve paper from facebook and others. theyve tried. other architectures where you basically have the lexical and semantic and maybe. you have the same kind of matrix representation that captures both whether the terms are exactly same or youre. talking about three or four different things. let me go ahead and see if we can do a few. more of these that stuff is available all week and i think maybe. a longer conversation with nabosa might be in order. yes. i thought so yeah are there few other questions. i have a question about local verses. global model you were talking about the way that i understand. it is that its a neural version of pseudo feedback basically so that you compare it to traditional method of trying to do pseudo feedback. yes actually ahmed and i think ahmed and bruce actually wrote a paper where they basically this wouldnt improve over pseudo relevance. this would not perform better than pseudo relevance feedback but they combined it with pseudo relevance feedback and showed that it improves on top but thats exactly correct so. this paper was written in the context of query expansion. but the core idea that fernando and i especially fernando was really trying to put forward is this whole point of learning a model specific to the situation you are in and so basically learning at runtime and going from there. but it was tested in that context. so i have a follow up. question related that i know. i would talk to you. later. when pseudo relevance feedback fails. it can fail badly. if you have a bad starting value youre going to do poorly. but it seems like if we use this in two places which is why bother use the pseudo relevance feedback. we have query logs that have click information which give you weights on which one should be the focus for any query weve seen before how to expand for that query or even in a session based case using the previous click documents in the session to expand for the next query where we know the thing. stays topically constrained have you looked at or thought about either of these maybe. i didnt get your question. the first one is for any query weve seen before. instead of looking. at top 1000 you can do a click based weighting click base. if you like to look at those documents thats going to get me a much more targeted at relevance and not have the failures of pseudo relevance feedback. so we can do expansion for any query that weve seen before. the second case is you can do same thing at a session based example which is the documents that somebody has clicked before current. query is there a case of what is relevant to this topic right and we know that people think topicly relevant within a session. again you are in a much better case by not doing pseudo relevance feedback. so two different answers. one is in terms of the first part of that. i think this whole point of local global analysis. if you think of the spectrum. the local analysis becomes really interesting when youre dealing with a query that has effectively no presence in your click data. so. for example the pecarovich land company you would expect that if there are even 50 documents in your index it might fetch those 50 documents and those 50 documents might be enough for you to learn a local representation and do something interesting. if you have enough presence of that in click streams you could imagine that the global representation might already capture a lot of it. of course. there might also be a sweet spot in between where it has some coverage from click data but not enough that your global representation does a good job so maybe you can use that yeah. you can definitely use that the second part about looking at previous documents in the same session again. ive not looked at it in the context of document ranking. but one of the things we chatted briefly before that. i was super interested but havent really made a breakthrough. there is when i was training the dssm model on spares of queries from session. the intuition that i really had was kind of starting to think about such sessions as effectively a path in an embedding space and if thats going to allow us to do something interesting in that direction right and that was trained on queries. but it could also be trained on queries and click documents and whatnot again. i havent done anything on that its something thats been on the back of my head. if anybodys interested to chat about that or has any ideas about that. i would love to talk about it. quick follow up to the follow up all right last question. the 2000 documents you are using helped your ranking or this was based on indri so he was fernando type data. not in here. you wouldnt use a full search engine right you would be incorporating. no. this was just based on retrieval and just something simpler model thats an important question by the way which im not going to touch upon is how you evaluate these models especially the deep neural models on top theres. this whole discussion about telescoping evaluation versus other things but i m going to refrain from that we can talk about it separately but its actually an interesting question all right thanks for speaking again. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    paragraph = token.replace(\"\\n\", \"\")\n",
    "    print(paragraph)\n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 different\\ differently\\ learning\\ query\\ neural models specifically\\ text\\ representation\\ representations\\ specific\\ big\\ today\\ todays\\ search\\ recent\\ recently\\ retrieval\\ retrieve\\ weve\\ online\\ traditional\\ main\\ possible\\ framework\\ frameworks\\ model\\ hes\\ right\\ general overview\\ generating manually designed\\ nice paper\\ papers\\ like\\ look\\ input\\ bhaskar\\ early\\ actually\\ office\\ mentioned\\ available\\ nick\\ area\\ areas\\ engine\n",
      "1 actually\\ featurize\\ featurization\\ features like\\ feature space\\ youre\\ different\\ start\\ starts\\ example\\ examples\\ spaces\\ seattle\\ representations\\ representation\\ words\\ word\\ embeddings\\ embedding\\ items\\ item\\ based\\ particular\\ seahawks\\ topic\\ vector\\ vectors\\ things\\ thing\\ zero\\ similar\\ similarity\\ wilson\\ people\\ explicitly\\ plus\\ come\\ coming\\ banana\\ neural network\\ small toy\\ long versus\\ basically\\ diagram\\ important\\ thats\\ pretty popular especially\\ factors\\ factorize\\ factorization\\ right\\ domes\\ dome\\ given\\ let\\ lets\n",
      "2 queries\\ ranking query\\ like document\\ actually\\ actual\\ different\\ documents\\ embeddings\\ models\\ model trained\\ right\\ youre\\ similar\\ similarity\\ train\\ training\\ suffixes\\ suffix\\ learned\\ learn\\ learning\\ embedding space\\ text\\ rank\\ relationship youve modelled\\ things\\ thing\\ tasks\\ task\\ prefix\\ prefixes\\ seattle\\ london\\ somebody\\ big\\ thats\\ chicago san\\ basically\\ based\\ pairs\\ pair\\ whats\\ doesnt\\ signal\\ cross\\ word\\ start\\ away\\ pretty\\ particular\\ expects\\ expect\\ examples\\ example\\ obviously\\ representation\\ representations\\ search\\ relationships\\ question\\ says\\ clock\\ goes\\ completion\\ complete\\ completely\\ randomly\\ type\\ types\\ network\\ nearest\\ kind\\ cheap\\ wrong\\ gives\\ university\\ given\\ pattern\n",
      "3 models\\ model\\ modeling\\ like\\ likely\\ query\\ queries\\ word\\ words\\ different\\ difference\\ differently\\ differ\\ document\\ documents\\ actually\\ termed\\ terms\\ term\\ training\\ trained\\ train\\ matching\\ matches\\ match\\ matched\\ right\\ basically\\ embeddings\\ embedding\\ learns\\ learn\\ learned\\ learning\\ things\\ thing\\ yes\\ actual good\\ text\\ youre\\ data\\ datas\\ representation\\ representations\\ based\\ vector\\ local\\ locally\\ thats\\ way\\ global\\ globally\\ completely\\ completion\\ work\\ worked\\ works\\ let\\ topical\\ topics\\ nyu\\ nyus\\ similar\\ similarity\\ similarities\\ sample\\ samples\\ sampling\\ specifically\\ specific\\ simple\\ having\\ inaudible\\ started\\ starts\\ start\\ starting\\ duet\\ context\\ big\\ field\\ space\\ spaces\\ relevant\\ relevance\\ favorite topic\\ wise\\ bit\\ sure\\ key\\ wikipedia\\ connected\\ mismatch\\ example\\ examples\\ multiple fields\\ run\\ running\\ runs\\ points\\ lines\\ line\\ relationship\\ relationships\\ frequency\\ think\\ thinking\\ doesnt\\ textural benching\\ retrieval\\ retrieves\\ exact\\ look\\ looking\\ looks\\ looked\\ window\\ pekarovic\\ typical\\ typically\\ loss\\ main point\\ cross entropy\\ neighbors\\ neighboring\\ neighbor\\ dealing\\ deal\\ given\\ matrix\\ cut\\ cutting\\ set\\ sets\\ yeah\\ albuquerque\\ ranking\\ rank\\ saying\\ says\\ okay\\ million\\ said\\ answer\\ slightly\\ performance\\ perform\\ memory\\ people\\ mean\\ meanings\\ means\\ real world\\ barack obama\\ suddenly\\ score\\ scores\\ single\\ potentially\\ yale\\ dot\\ youve\\ dssm\\ input\\ inputs\\ craswell\\ second\\ nick\\ passages\\ passage\\ diagram\\ research\\ balance moves\\ inherently\\ effect\\ effectively\\ effective\\ shows\\ showed\\ ambiguous\\ moving\\ sees\\ seeing\\ instead\\ intern\\ nalisnick\\ web\\ quick overview\\ zero\\ ahmed\\ rarely\n",
      "4 query\\ queries\\ different\\ difference\\ clicked\\ actually\\ thats\\ neural models\\ documents\\ document\\ rank model\\ yes\\ field like click\\ based\\ base\\ feedback\\ questions\\ recommendations\\ use\\ youre\\ global\\ local\\ thinking\\ think\\ topic right\\ learning\\ learn\\ learned\\ learns\\ interesting\\ interested\\ actively recommending\\ matches\\ matching\\ match\\ useful especially\\ land\\ note\\ convolution\\ convolutional\\ network\\ precise\\ good question\\ versus later\\ case\\ topically\\ things\\ thing\\ wouldnt\\ exactly\\ evaluate\\ evaluation\\ length\\ lengths\\ ranking\\ relevance\\ relevant\\ representation\\ basically\\ level\\ serve\\ starting\\ better\\ engine\\ architectures\\ architectural\\ fields\\ chatted briefly\\ chat\\ personal\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for token in tokens:\n",
    "#     print(token.replace(\"\\n\", \" \"))\n",
    "    print(i, keywords(token.replace(\"\\n\", \" \"), ratio=0.3).replace(\"\\n\", \"\\ \"))\n",
    "    i += 1\n",
    "    #     print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "ii = 0\n",
    "for j in range(1, len(data)):\n",
    "    for i in range(ii, len(tokens)):\n",
    "        if data[j]['text'].replace(\"\\n\", \" \")\\\n",
    "                .split('.')[0] in tokens[i].replace(\"\\n\", \"\")\\\n",
    "                .replace(\".\", \"\"):\n",
    "            timestamps += [data[j]['start']]\n",
    "            ii += 1\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_min = [str(int(x // 3600))+ ':' + \n",
    "                  str(int(x // 60 % 60)) + ':' + \\\n",
    "                  str(int(x % 60)) for x in timestamps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.691 0:0:29\n",
      "338.761 0:5:38\n",
      "1045.436 0:17:25\n",
      "1710.981 0:28:30\n",
      "3233.761 0:53:53\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(timestamps_min)):\n",
    "    print(timestamps[j], timestamps_min[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0:0:29 different\\ differently\\ neural\\ learning\\ query\\ text\\ representation\\ representations\n",
      "1 0:5:38 actually\\ feature\\ featurize\\ featurization\\ features like\\ youre\\ different\\ start\\ starts\\ example\\ examples\n",
      "2 0:17:25 query\\ queries\\ like document\\ actually\\ actual\\ different\\ documents\\ embedding\\ embeddings\\ modelled\\ model\\ models\\ right\\ youre\\ similar\\ similarity\n",
      "3 0:28:30 models\\ model\\ modeling\\ like\\ likely\\ query\\ queries\\ word\\ words\\ different\\ difference\\ differently\\ differ\\ document\\ documents\\ actually\\ actual\\ termed\\ terms\\ term\\ training\\ trained\\ train\\ matching\\ matches\\ match\\ matched\\ right\\ basically\\ embeddings\\ embedding\\ learns\\ learn\\ learned\\ learning\\ things\\ thing\\ yes\\ text\\ youre\\ data\\ datas\\ representation\\ representations\\ based\n",
      "4 0:53:53 model\\ models\\ query\\ queries\\ different\\ difference\\ clicked\\ actually\\ thats\\ like click\\ documents\\ document\\ yes\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for j in range(min(len(tokens), len(timestamps_min))):\n",
    "#     print(token.replace(\"\\n\", \" \"))\n",
    "    print(i, timestamps_min[i], keywords(tokens[j] .replace(\n",
    "                    \"\\n\", \" \"), ratio=0.05).replace(\"\\n\", \"\\ \"))\n",
    "    i += 1\n",
    "    #     print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
